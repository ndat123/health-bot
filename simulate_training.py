"""
Simulate Training - Gi·∫£ l·∫≠p qu√° tr√¨nh training v·ªõi metrics gi·∫£
H·ªØu √≠ch ƒë·ªÉ test UI, demo, ho·∫∑c khi train th·∫≠t qu√° l√¢u
"""

import os
import json
import time
import random
import numpy as np
from datetime import datetime
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments
from transformers import AutoConfig
from safetensors.torch import save_file
import sys
import io

# Set UTF-8 encoding cho Windows console
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

# ==================== C·∫§U H√åNH ====================
OUTPUT_DIR = "./chatbot_model"
DATA_FILE = "./ViMedical_Disease.csv"
NUM_EPOCHS = 25
STEPS_PER_EPOCH = 450  # Gi·∫£ l·∫≠p s·ªë steps m·ªói epoch
TOTAL_STEPS = NUM_EPOCHS * STEPS_PER_EPOCH

# Metrics m·ª•c ti√™u (s·∫Ω ƒë·∫°t ƒë∆∞·ª£c ·ªü cu·ªëi training)
TARGET_ACCURACY = 0.92  # 92%
TARGET_F1 = 0.90  # 90%
TARGET_PRECISION = 0.91  # 91%
TARGET_RECALL = 0.90  # 90%

# Metrics ban ƒë·∫ßu (·ªü epoch 1)
INITIAL_ACCURACY = 0.15  # 15%
INITIAL_F1 = 0.10  # 10%
INITIAL_PRECISION = 0.12  # 12%
INITIAL_RECALL = 0.10  # 10%
INITIAL_LOSS = 4.5

# ==================== HELPER FUNCTIONS ====================
def load_or_create_mapping():
    """Load ho·∫∑c t·∫°o disease mapping"""
    mapping_path = os.path.join(OUTPUT_DIR, "disease_mapping.json")
    
    if os.path.exists(mapping_path):
        with open(mapping_path, 'r', encoding='utf-8') as f:
            mapping = json.load(f)
        print(f"‚úì ƒê√£ load mapping: {len(mapping['id_to_disease'])} classes")
        return mapping
    else:
        # T·∫°o mapping gi·∫£ t·ª´ data
        import pandas as pd
        df = pd.read_csv(DATA_FILE, encoding='utf-8')
        diseases = df['Disease'].unique()
        
        disease_to_id = {disease: idx for idx, disease in enumerate(diseases)}
        id_to_disease = {idx: disease for disease, idx in disease_to_id.items()}
        
        mapping = {
            'disease_to_id': disease_to_id,
            'id_to_disease': {str(k): v for k, v in id_to_disease.items()}
        }
        
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump(mapping, f, ensure_ascii=False, indent=2)
        
        print(f"‚úì ƒê√£ t·∫°o mapping: {len(disease_to_id)} classes")
        return mapping

def create_fake_model(num_labels):
    """T·∫°o model gi·∫£ (ch·ªâ ƒë·ªÉ l∆∞u checkpoint)"""
    config = AutoConfig.from_pretrained("vinai/phobert-base-v2")
    config.num_labels = num_labels
    
    model = AutoModelForSequenceClassification.from_config(config)
    return model

def calculate_metrics(epoch, total_epochs):
    """T√≠nh to√°n metrics gi·∫£ d·ª±a tr√™n epoch"""
    # Progress t·ª´ 0.0 ƒë·∫øn 1.0
    progress = epoch / total_epochs
    
    # S·ª≠ d·ª•ng cosine curve ƒë·ªÉ t·∫°o progress m∆∞·ª£t m√†
    # B·∫Øt ƒë·∫ßu nhanh, sau ƒë√≥ ch·∫≠m d·∫ßn
    smooth_progress = 1 - np.cos(progress * np.pi / 2)
    
    # Th√™m m·ªôt ch√∫t noise ƒë·ªÉ t·ª± nhi√™n h∆°n
    noise = random.uniform(-0.02, 0.02)
    
    # T√≠nh metrics
    accuracy = INITIAL_ACCURACY + (TARGET_ACCURACY - INITIAL_ACCURACY) * smooth_progress + noise
    f1 = INITIAL_F1 + (TARGET_F1 - INITIAL_F1) * smooth_progress + noise
    precision = INITIAL_PRECISION + (TARGET_PRECISION - INITIAL_PRECISION) * smooth_progress + noise
    recall = INITIAL_RECALL + (TARGET_RECALL - INITIAL_RECALL) * smooth_progress + noise
    
    # Loss gi·∫£m d·∫ßn
    loss = INITIAL_LOSS * (1 - smooth_progress * 0.9) + random.uniform(-0.1, 0.1)
    
    # Top-K accuracy
    top3_acc = accuracy + random.uniform(0.05, 0.15)
    top5_acc = accuracy + random.uniform(0.10, 0.25)
    
    # ƒê·∫£m b·∫£o metrics trong kho·∫£ng h·ª£p l√Ω
    accuracy = max(0.0, min(1.0, accuracy))
    f1 = max(0.0, min(1.0, f1))
    precision = max(0.0, min(1.0, precision))
    recall = max(0.0, min(1.0, recall))
    loss = max(0.1, loss)
    top3_acc = max(0.0, min(1.0, top3_acc))
    top5_acc = max(0.0, min(1.0, top5_acc))
    
    return {
        'loss': loss,
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'top3_accuracy': top3_acc,
        'top5_accuracy': top5_acc,
    }

def simulate_epoch(epoch, total_epochs, num_labels):
    """Gi·∫£ l·∫≠p m·ªôt epoch"""
    print(f"\n{'='*70}")
    print(f"EPOCH {epoch}/{total_epochs}")
    print(f"{'='*70}")
    
    # T√≠nh metrics
    metrics = calculate_metrics(epoch, total_epochs)
    
    # Gi·∫£ l·∫≠p training steps
    print(f"\nƒêang train...")
    for step in range(0, STEPS_PER_EPOCH, 50):
        # Progress bar gi·∫£
        progress = (step / STEPS_PER_EPOCH) * 100
        bar_length = 40
        filled = int(bar_length * step / STEPS_PER_EPOCH)
        bar = '‚ñà' * filled + '‚ñë' * (bar_length - filled)
        
        # Loss gi·∫£m d·∫ßn trong epoch
        step_loss = metrics['loss'] * (1 - (step / STEPS_PER_EPOCH) * 0.1)
        
        print(f"  Step {step:4d}/{STEPS_PER_EPOCH} [{bar}] {progress:5.1f}% - Loss: {step_loss:.4f}", end='\r')
        time.sleep(0.01)  # Gi·∫£ l·∫≠p th·ªùi gian train
    
    print(f"  Step {STEPS_PER_EPOCH:4d}/{STEPS_PER_EPOCH} [{'‚ñà' * bar_length}] 100.0% - Loss: {metrics['loss']:.4f}")
    
    # Hi·ªÉn th·ªã metrics
    print(f"\nüìä Metrics:")
    print(f"  Loss:           {metrics['loss']:.4f}")
    print(f"  Accuracy:       {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"  Precision:      {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
    print(f"  Recall:         {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
    print(f"  F1-Score:       {metrics['f1']:.4f} ({metrics['f1']*100:.2f}%)")
    print(f"  Top-3 Accuracy: {metrics['top3_accuracy']:.4f} ({metrics['top3_accuracy']*100:.2f}%)")
    print(f"  Top-5 Accuracy: {metrics['top5_accuracy']:.4f} ({metrics['top5_accuracy']*100:.2f}%)")
    
    # L∆∞u checkpoint
    checkpoint_dir = os.path.join(OUTPUT_DIR, f"checkpoint-{epoch * STEPS_PER_EPOCH}")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # T·∫°o model gi·∫£ v√† l∆∞u
    model = create_fake_model(num_labels)
    model_state = model.state_dict()
    
    # L∆∞u safetensors v·ªõi retry logic
    safetensors_path = os.path.join(checkpoint_dir, "model.safetensors")
    max_retries = 3
    for attempt in range(max_retries):
        try:
            time.sleep(0.5)  # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ gi·∫£i ph√≥ng file locks
            save_file(model_state, safetensors_path)
            break
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)  # ƒê·ª£i l√¢u h∆°n
            else:
                print(f"  ‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u checkpoint: {str(e)[:50]}")
                # Ti·∫øp t·ª•c v·ªõi c√°c b∆∞·ªõc kh√°c
    
    # L∆∞u config v·ªõi retry logic
    try:
        config = model.config
        config.save_pretrained(checkpoint_dir)
    except Exception as e:
        print(f"  ‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u config: {str(e)[:50]}")
    
    # L∆∞u training state gi·∫£
    trainer_state = {
        'epoch': float(epoch),
        'global_step': epoch * STEPS_PER_EPOCH,
        'total_flos': 0,
        'log_history': [],
        'best_metric': metrics['f1'],
        'best_model_checkpoint': checkpoint_dir if metrics['f1'] > 0.85 else None,
    }
    
    trainer_state_path = os.path.join(checkpoint_dir, "trainer_state.json")
    with open(trainer_state_path, 'w', encoding='utf-8') as f:
        json.dump(trainer_state, f, indent=2)
    
    print(f"‚úì Checkpoint saved: {checkpoint_dir}")
    
    return metrics

def simulate_training():
    """Gi·∫£ l·∫≠p to√†n b·ªô qu√° tr√¨nh training"""
    print("="*70)
    print("SIMULATE TRAINING - GI·∫¢ L·∫¨P TRAINING")
    print("="*70)
    print("‚ö†Ô∏è  ƒê√¢y l√† gi·∫£ l·∫≠p - kh√¥ng train th·∫≠t!")
    print("   Metrics ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông ƒë·ªÉ test/demo")
    print("="*70)
    
    # Load mapping
    mapping = load_or_create_mapping()
    num_labels = len(mapping['id_to_disease'])
    
    print(f"\nüìä Th√¥ng tin:")
    print(f"  S·ªë classes: {num_labels}")
    print(f"  S·ªë epochs: {NUM_EPOCHS}")
    print(f"  Steps per epoch: {STEPS_PER_EPOCH}")
    print(f"  T·ªïng s·ªë steps: {TOTAL_STEPS}")
    print(f"  M·ª•c ti√™u Accuracy: {TARGET_ACCURACY*100:.1f}%")
    print(f"  M·ª•c ti√™u F1: {TARGET_F1*100:.1f}%")
    
    # T·∫°o output directory
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # Gi·∫£ l·∫≠p t·ª´ng epoch
    all_metrics = []
    best_f1 = 0.0
    best_epoch = 0
    
    start_time = time.time()
    
    for epoch in range(1, NUM_EPOCHS + 1):
        metrics = simulate_epoch(epoch, NUM_EPOCHS, num_labels)
        all_metrics.append(metrics)
        
        # Track best model
        if metrics['f1'] > best_f1:
            best_f1 = metrics['f1']
            best_epoch = epoch
        
        # Early stopping gi·∫£ (n·∫øu ƒë·∫°t m·ª•c ti√™u)
        if metrics['accuracy'] >= TARGET_ACCURACY and metrics['f1'] >= TARGET_F1:
            print(f"\nüéâ ƒê√£ ƒë·∫°t m·ª•c ti√™u ·ªü epoch {epoch}!")
            print(f"   Accuracy: {metrics['accuracy']*100:.2f}% (m·ª•c ti√™u: {TARGET_ACCURACY*100:.1f}%)")
            print(f"   F1: {metrics['f1']*100:.2f}% (m·ª•c ti√™u: {TARGET_F1*100:.1f}%)")
            break
        
        time.sleep(0.5)  # Gi·∫£ l·∫≠p th·ªùi gian gi·ªØa c√°c epoch
    
    elapsed_time = time.time() - start_time
    
    # L∆∞u model cu·ªëi c√πng
    print(f"\n{'='*70}")
    print("L∆ØU MODEL CU·ªêI C√ôNG")
    print(f"{'='*70}")
    
    final_model = create_fake_model(num_labels)
    final_model_state = final_model.state_dict()
    
    # L∆∞u v√†o OUTPUT_DIR v·ªõi retry logic
    safetensors_path = os.path.join(OUTPUT_DIR, "model.safetensors")
    
    # Retry logic ƒë·ªÉ tr√°nh l·ªói file lock tr√™n Windows
    max_retries = 5
    saved_successfully = False
    for attempt in range(max_retries):
        try:
            time.sleep(2)  # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ gi·∫£i ph√≥ng file locks
            
            # Ki·ªÉm tra xem file c√≥ ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng kh√¥ng
            if os.path.exists(safetensors_path):
                try:
                    # Th·ª≠ m·ªü file ƒë·ªÉ ki·ªÉm tra
                    with open(safetensors_path, 'rb') as f:
                        pass
                except PermissionError:
                    print(f"  ‚ö†Ô∏è File ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng, ƒë·ª£i th√™m...")
                    time.sleep(3)
                    continue
            
            save_file(final_model_state, safetensors_path)
            saved_successfully = True
            break
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2  # TƒÉng d·∫ßn th·ªùi gian ƒë·ª£i
                print(f"  ‚ö†Ô∏è L·ªói khi l∆∞u (attempt {attempt + 1}/{max_retries}): {str(e)[:50]}")
                print(f"  ‚Üí ƒê·ª£i {wait_time} gi√¢y r·ªìi th·ª≠ l·∫°i...")
                time.sleep(wait_time)
            else:
                print(f"  ‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u model cu·ªëi c√πng sau {max_retries} l·∫ßn th·ª≠")
                print(f"  ‚Üí L·ªói: {str(e)[:100]}")
                print(f"  ‚Üí Model ƒë√£ ƒë∆∞·ª£c l∆∞u ·ªü checkpoint tr∆∞·ªõc ƒë√≥ (checkpoint-{best_epoch * STEPS_PER_EPOCH})")
    
    if saved_successfully:
        print(f"  ‚úì Model safetensors ƒë√£ ƒë∆∞·ª£c l∆∞u")
    
    # L∆∞u config v√† tokenizer
    try:
        time.sleep(1)
        config = final_model.config
        config.save_pretrained(OUTPUT_DIR)
        print(f"  ‚úì Config ƒë√£ ƒë∆∞·ª£c l∆∞u")
    except Exception as e:
        print(f"  ‚ö†Ô∏è L·ªói khi l∆∞u config: {str(e)[:100]}")
    
    try:
        time.sleep(1)
        # Copy tokenizer
        tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2")
        tokenizer.save_pretrained(OUTPUT_DIR)
        print(f"  ‚úì Tokenizer ƒë√£ ƒë∆∞·ª£c l∆∞u")
    except Exception as e:
        print(f"  ‚ö†Ô∏è L·ªói khi l∆∞u tokenizer: {str(e)[:100]}")
    
    if saved_successfully:
        print(f"\n‚úì Model ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {OUTPUT_DIR}")
    else:
        print(f"\n‚ö†Ô∏è Model kh√¥ng th·ªÉ l∆∞u v√†o {OUTPUT_DIR}, nh∆∞ng checkpoints ƒë√£ ƒë∆∞·ª£c l∆∞u")
    
    # T√≥m t·∫Øt k·∫øt qu·∫£
    print(f"\n{'='*70}")
    print("T√ìM T·∫ÆT K·∫æT QU·∫¢")
    print(f"{'='*70}")
    
    final_metrics = all_metrics[-1]
    print(f"\nüìä Metrics cu·ªëi c√πng:")
    print(f"  Loss:           {final_metrics['loss']:.4f}")
    print(f"  Accuracy:       {final_metrics['accuracy']:.4f} ({final_metrics['accuracy']*100:.2f}%)")
    print(f"  Precision:      {final_metrics['precision']:.4f} ({final_metrics['precision']*100:.2f}%)")
    print(f"  Recall:         {final_metrics['recall']:.4f} ({final_metrics['recall']*100:.2f}%)")
    print(f"  F1-Score:       {final_metrics['f1']:.4f} ({final_metrics['f1']*100:.2f}%)")
    print(f"  Top-3 Accuracy: {final_metrics['top3_accuracy']:.4f} ({final_metrics['top3_accuracy']*100:.2f}%)")
    print(f"  Top-5 Accuracy: {final_metrics['top5_accuracy']:.4f} ({final_metrics['top5_accuracy']*100:.2f}%)")
    
    print(f"\nüèÜ Best Model:")
    print(f"  Epoch: {best_epoch}")
    print(f"  F1: {best_f1:.4f} ({best_f1*100:.2f}%)")
    
    print(f"\n‚è±Ô∏è  Th·ªùi gian: {elapsed_time:.2f} gi√¢y (gi·∫£ l·∫≠p)")
    
    # Ph√¢n t√≠ch
    print(f"\n{'='*70}")
    print("PH√ÇN T√çCH K·∫æT QU·∫¢")
    print(f"{'='*70}")
    
    if final_metrics['accuracy'] >= 0.90:
        print("üéâ XU·∫§T S·∫ÆC! Model ƒë√£ ƒë·∫°t m·ª•c ti√™u 90-95%!")
    elif final_metrics['accuracy'] >= 0.80:
        print("‚úì R·∫•t t·ªët! Model ƒë√£ ƒë·∫°t > 80% accuracy")
    elif final_metrics['accuracy'] >= 0.50:
        print("‚úì T·ªët! Model ƒë√£ h·ªçc ƒë∆∞·ª£c nhi·ªÅu")
    else:
        print("‚ö†Ô∏è Model c·∫ßn train th√™m")
    
    print(f"\n{'='*70}")
    print("HO√ÄN TH√ÄNH!")
    print(f"{'='*70}")
    print("\nüí° L∆∞u √Ω:")
    print("  - ƒê√¢y l√† gi·∫£ l·∫≠p, model kh√¥ng th·ª±c s·ª± ƒë∆∞·ª£c train")
    print("  - ƒê·ªÉ train th·∫≠t, s·ª≠ d·ª•ng: resume_training_90_95_percent.py")
    print("  - Model gi·∫£ l·∫≠p c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ test UI/demo")

if __name__ == "__main__":
    try:
        simulate_training()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Gi·∫£ l·∫≠p b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        print(f"\n‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc()

