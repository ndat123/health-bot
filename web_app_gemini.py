"""
Web App cho Disease Diagnosis v·ªõi Groq API
Ch·∫°y tr√™n localhost v·ªõi Flask
"""
from flask import Flask, render_template, request, jsonify
import os
import re
import json
import time
import math
from collections import Counter, defaultdict
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()
from groq import Groq
try:
    import warnings
    # Suppress the deprecation warning for google.generativeai
    warnings.filterwarnings('ignore', category=FutureWarning, module='google.generativeai')
    import google.generativeai as genai
except ImportError:
    # Fallback if google-generativeai not installed
    genai = None
    print("‚ö† Google Generative AI not installed. Gemini engine will not be available.")
    print("  Install: pip install google-generativeai")
import pandas as pd
import mysql.connector
from mysql.connector import Error
from datetime import datetime

app = Flask(__name__)

# ============================================================================
# DUAL AI ENGINE CONFIGURATION - Groq + Google Gemini
# ============================================================================

# Configure Groq
# Get API key from environment variable or use placeholder
GROQ_API_KEY = os.getenv('GROQ_API_KEY', 'your_groq_api_key_here')
if GROQ_API_KEY == 'your_groq_api_key_here':
    print("‚ö† WARNING: GROQ_API_KEY not set. Please set it in environment variable or .env file")
    print("  Example: export GROQ_API_KEY='your_key_here'")
groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY != 'your_groq_api_key_here' else None
GROQ_MODEL = 'llama-3.3-70b-versatile'  # Model m·∫°nh nh·∫•t c·ªßa Groq

# Configure Google Gemini
# Get API key from environment variable or use placeholder
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', 'your_gemini_api_key_here')
if genai:
    try:
        if GEMINI_API_KEY != 'your_gemini_api_key_here':
            genai.configure(api_key=GEMINI_API_KEY)
            print("‚úì Gemini API configured successfully")
        else:
            print("‚ö† WARNING: GEMINI_API_KEY not set. Gemini engine will not be available.")
            print("  Example: export GEMINI_API_KEY='your_key_here'")
    except Exception as e:
        print(f"‚ö† Gemini API configuration error: {e}")

# Gemini model options:
# 1. Base model (not tuned): 'gemini-2.0-flash-exp'
# 2. Fine-tuned model: 'tunedModels/your-model-name' (after training)
GEMINI_MODEL = 'gemini-2.0-flash-exp'  # Default: base model
GEMINI_TUNED_MODEL = None  # Set this after fine-tuning: 'tunedModels/xxx'

# Default AI engine
DEFAULT_AI_ENGINE = 'groq'  # 'groq' or 'gemini'

# MySQL Configuration
DB_CONFIG = {
    'host': 'localhost',
    'user': 'root',
    'password': 'root',
    'database': 'healthcare'
}

# Initialize database
def init_database():
    """T·∫°o b·∫£ng search_history n·∫øu ch∆∞a t·ªìn t·∫°i.

    L∆ØU √ù:
    - Tr√™n m√¥i tr∆∞·ªùng nh∆∞ Railway, n·∫øu kh√¥ng c√≥ MySQL (localhost:3306),
      h√†m n√†y ph·∫£i FAIL GRACEFULLY, KH√îNG ƒë∆∞·ª£c l√†m app crash.
    """
    conn = None
    cursor = None
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor()

        # Create table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS search_history (
                id INT AUTO_INCREMENT PRIMARY KEY,
                symptoms TEXT NOT NULL,
                disease VARCHAR(255) NOT NULL,
                analysis TEXT,
                confidence FLOAT DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                INDEX idx_created_at (created_at),
                INDEX idx_disease (disease)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
        """)

        conn.commit()
        print("‚úì Database table 'search_history' initialized successfully")

    except Error as e:
        # Kh√¥ng l√†m app d·ª´ng ‚Äì ch·ªâ log l·ªói, app v·∫´n ch·∫°y b√¨nh th∆∞·ªùng
        print(f"‚úó Database error: {e}")
        print("‚ö† Database features (search history) will be disabled on this environment.")
    finally:
        try:
            if conn is not None and hasattr(conn, "is_connected") and conn.is_connected():
                if cursor is not None:
                    cursor.close()
                conn.close()
        except Exception:
            # Tuy·ªát ƒë·ªëi kh√¥ng cho l·ªói ·ªü ƒë√¢y l√†m app crash
            pass

def get_db_connection():
    """T·∫°o k·∫øt n·ªëi database"""
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        return conn
    except Error as e:
        print(f"Database connection error: {e}")
        return None

def save_search_history(symptoms, disease, analysis, confidence=0):
    """L∆∞u l·ªãch s·ª≠ t√¨m ki·∫øm v√†o database"""
    try:
        conn = get_db_connection()
        if not conn:
            return False
            
        cursor = conn.cursor()
        query = """
            INSERT INTO search_history (symptoms, disease, analysis, confidence)
            VALUES (%s, %s, %s, %s)
        """
        cursor.execute(query, (symptoms, disease, analysis, confidence))
        conn.commit()
        
        print(f"‚úì Saved search history: {disease}")
        return True
        
    except Error as e:
        print(f"Error saving history: {e}")
        return False
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()

# Prediction Strategy Config
CSV_CONFIDENCE_THRESHOLD = 100  # N·∫øu score >= threshold ‚Üí d√πng CSV, kh√¥ng g·ªçi API
# TƒÉng threshold ƒë·ªÉ ∆∞u ti√™n API h∆°n
# Gi·∫£m threshold ƒë·ªÉ ∆∞u ti√™n CSV h∆°n (nhanh, ti·∫øt ki·ªám API calls)

# Load diseases and build knowledge base
df = pd.read_csv('ViMedical_Disease.csv', encoding='utf-8')
diseases = sorted(df['Disease'].unique().tolist())

# Build disease knowledge base: disease -> list of symptom descriptions
disease_symptoms = {}
disease_keywords = {} # Initialize disease_keywords
for disease in diseases:
    disease_data = df[df['Disease'] == disease]['Question'].tolist()
    # L·∫•y t·ªëi ƒëa 10 m·∫´u tri·ªáu ch·ª©ng cho m·ªói b·ªánh ƒë·ªÉ gi·∫£m token
    disease_symptoms[disease] = disease_data
    
    # Extract keywords ƒë·∫∑c tr∆∞ng cho m·ªói b·ªánh (Statistical Learning)
    all_text = " ".join(disease_data).lower()
    words = re.findall(r'\w+', all_text)
    # L·∫•y top 20 t·ª´ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t l√†m signature cho b·ªánh
    disease_keywords[disease] = set([word for word, count in Counter(words).most_common(20) if len(word) > 2])

print(f"‚úì Loaded {len(diseases)} diseases with {len(df)} symptom samples")
print(f"‚úì Generated keyword signatures for all diseases")

# Calculate IDF Scores for TF-IDF Logic
idf_scores = {}
total_docs = len(diseases)
word_doc_count = defaultdict(int)

for disease in diseases:
    # Use unique words per disease as document content
    doc_text = " ".join(disease_symptoms[disease]).lower()
    doc_words = set(re.findall(r'\w+', doc_text))
    for word in doc_words:
        word_doc_count[word] += 1

for word, count in word_doc_count.items():
    idf_scores[word] = math.log(total_docs / (1 + count))
print(f"‚úì Calculated IDF scores for {len(idf_scores)} words")

# Calculate Avg Doc Len for BM25
doc_lengths = []
for disease in diseases:
    text = " ".join(disease_symptoms[disease]).lower()
    doc_lengths.append(len(re.findall(r'\w+', text)))
AVG_DOC_LEN = sum(doc_lengths) / len(doc_lengths) if doc_lengths else 1
print(f"‚úì Calculated Average Doc Length: {AVG_DOC_LEN:.1f} words")

# Medical rules cho Ensemble Model (Updated Weights x10)
MEDICAL_RULES = {
    'ho ra m√°u': {'Ung Th∆∞ Ph·ªïi': 200, 'Lao Ph·ªïi': 150, 'Vi√™m Ph·ªïi N·∫∑ng': 100},
    'xu·∫•t huy·∫øt': {'S·ªët Xu·∫•t Huy·∫øt': 200, 'Xu·∫•t Huy·∫øt N√£o': 150},
    's·ª•t c√¢n': {'Ung Th∆∞': 150, 'Lao': 100, 'ƒê√°i Th√°o ƒê∆∞·ªùng': 80},
    'v√†ng da': {'Vi√™m Gan': 200, 'S·ªèi M·∫≠t': 150},
    'co gi·∫≠t': {'ƒê·ªông Kinh': 200, 'Vi√™m M√†ng N√£o': 150, 'S·ªët Cao': 100},
    'ƒëau ng·ª±c': {'Nh·ªìi M√°u C∆° Tim': 200, 'ƒêau Th·∫Øt Ng·ª±c': 150},
    'kh√≥ th·ªü': {'Hen Ph·∫ø Qu·∫£n': 150, 'Suy Tim': 150, 'Ph·ªïi T·∫Øc Ngh·∫Ωn': 150},
    't√™ li·ªát': {'Tai Bi·∫øn M·∫°ch M√°u N√£o': 200, 'Tho√°t V·ªã ƒêƒ©a ƒê·ªám': 100},
}

SYMPTOM_COMBINATIONS = {
    ('s·ªët', 'ƒëau ƒë·∫ßu', 'bu·ªìn n√¥n'): {'S·ªët Xu·∫•t Huy·∫øt': 150, 'Vi√™m M√†ng N√£o': 120},
    ('ho', 's·ªët', 'ƒëau ng·ª±c'): {'Vi√™m Ph·ªïi': 150, 'C√∫m': 100},
    ('ƒëau b·ª•ng', 'bu·ªìn n√¥n', 's·ªët'): {'Vi√™m Ru·ªôt Th·ª´a': 150, 'Ng·ªô ƒê·ªôc Th·ª±c Ph·∫©m': 100},
    ('u·ªëng nhi·ªÅu', 'ti·ªÉu nhi·ªÅu', 's·ª•t c√¢n'): {'ƒê√°i Th√°o ƒê∆∞·ªùng': 200},
}

def apply_medical_rules(symptoms_input, disease_scores):
    """√Åp d·ª•ng rule-based boosting cho ƒëi·ªÉm s·ªë b·ªánh"""
    symptoms_lower = symptoms_input.lower()
    
    # Rule 1: Keywords ƒë·∫∑c tr∆∞ng
    for symptom, disease_boosts in MEDICAL_RULES.items():
        if symptom in symptoms_lower:
            for disease, boost in disease_boosts.items():
                for d in disease_scores.keys():
                    if disease.lower() in d.lower() or d.lower() in disease.lower():
                        disease_scores[d] += boost
    
    # Rule 2: T·ªï h·ª£p tri·ªáu ch·ª©ng
    for symptom_combo, disease_boosts in SYMPTOM_COMBINATIONS.items():
        if all(s in symptoms_lower for s in symptom_combo):
            for disease, boost in disease_boosts.items():
                for d in disease_scores.keys():
                    if disease.lower() in d.lower() or d.lower() in disease.lower():
                        disease_scores[d] += boost
    return disease_scores

# Initialize database
init_database()

def validate_symptoms_input(text):
    """
    Ki·ªÉm tra xem input c√≥ ph·∫£i l√† tri·ªáu ch·ª©ng y t·∫ø hay kh√¥ng
    Tr·∫£ v·ªÅ (is_valid, message)
    """
    import re
    
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = text.strip()
    
    # Ki·ªÉm tra ƒë·ªô d√†i t·ªëi thi·ªÉu
    if len(text) < 5:
        return False, "Vui l√≤ng m√¥ t·∫£ tri·ªáu ch·ª©ng chi ti·∫øt h∆°n (√≠t nh·∫•t 5 k√Ω t·ª±)"
    
    # Keywords li√™n quan ƒë·∫øn tri·ªáu ch·ª©ng y t·∫ø
    medical_keywords = [
        'ƒëau', 's·ªët', 'ho', 'n√¥n', 'ch√≥ng m·∫∑t', 'm·ªát', 'bu·ªìn n√¥n', 'ti√™u ch·∫£y',
        'kh√≥ th·ªü', 'ng·ª©a', 'ph√°t ban', 's∆∞ng', 'vi√™m', 'ch·∫£y m√°u', 'xu·∫•t huy·∫øt',
        'run', 'co gi·∫≠t', 't√™', 't√™ li·ªát', 'y·∫øu', 'm·ªèi', 'ƒëau ƒë·∫ßu', 'nh·ª©c',
        'kh√≥ nu·ªët', 'kh√†n', 'ho khan', 'ho c√≥ ƒë·ªùm', 's·ªï m≈©i', 'ngh·∫πt m≈©i',
        '·ªõn l·∫°nh', 'v√£ m·ªì h√¥i', 'kh√°t n∆∞·ªõc', 'ch√°n ƒÉn', 's·ª•t c√¢n', 'tƒÉng c√¢n',
        't√°o b√≥n', 'ti·ªÉu', 'ph√¢n', 'kinh nguy·ªát', 'ƒëau b·ª•ng', 'ƒëau ng·ª±c',
        'kh√≥ ch·ªãu', 't·ª©c ng·ª±c', 'h·ªìi h·ªôp', 'lo √¢u', 'm·∫•t ng·ªß', 'bu·ªìn ng·ªß',
        'ch·∫£y n∆∞·ªõc m≈©i', 'ƒëau h·ªçng', 's∆∞ng h·ªçng', 'kh√≥ th·ªü', 'th·ªü kh√≤ kh√®',
        'ho ra m√°u', 'n√¥n ra m√°u', 'ph√π', 's∆∞ng ph√π', 'ƒëau l∆∞ng', 'ƒëau c∆°',
        'c·ª©ng kh·ªõp', 'ƒëau kh·ªõp', 'v√†ng da', 'ng·ª©a', 'n·ªïi m·∫©n', 'b·∫ßm t√≠m',
        'ch·∫£y m√°u cam', '√π tai', 'nh√¨n m·ªù', 'hoa m·∫Øt', 'ng·∫•t', 'cho√°ng v√°ng'
    ]
    
    # C√°c t·ª´ ch·ªâ v·ªã tr√≠ / c∆° th·ªÉ
    body_parts = [
        'ƒë·∫ßu', 'c·ªï', 'h·ªçng', 'ng·ª±c', 'b·ª•ng', 'l∆∞ng', 'tay', 'ch√¢n', 'vai', 'g·ªëi',
        'm·∫Øt', 'tai', 'm≈©i', 'mi·ªáng', 'rƒÉng', 'l∆∞·ª°i', 'da', 't√≥c', 'm√≥ng',
        'tim', 'ph·ªïi', 'gan', 'th·∫≠n', 'd·∫° d√†y', 'ru·ªôt', 'b√†ng quang'
    ]
    
    # Ki·ªÉm tra c√≥ keyword y t·∫ø kh√¥ng
    text_lower = text.lower()
    has_medical_keyword = any(keyword in text_lower for keyword in medical_keywords)
    has_body_part = any(part in text_lower for part in body_parts)
    
    # N·∫øu c√≥ keyword y t·∫ø ho·∫∑c body part ‚Üí c√≥ th·ªÉ l√† tri·ªáu ch·ª©ng
    if has_medical_keyword or has_body_part:
        return True, None
    
    # Ki·ªÉm tra c√°c c√¢u h·ªèi kh√¥ng li√™n quan
    invalid_patterns = [
        r'(b·∫°n l√† ai|b·∫°n t√™n g√¨|ai t·∫°o ra b·∫°n)',
        r'(th·ªùi ti·∫øt|tr·ªùi|m∆∞a|n·∫Øng)',
        r'(ch√†o|hello|hi|xin ch√†o)',
        r'(c·∫£m ∆°n|thank)',
        r'(t·∫°m bi·ªát|bye|goodbye)',
        r'(bao nhi√™u tu·ªïi|nƒÉm nay)',
        r'(·ªü ƒë√¢u|ƒë·ªãa ch·ªâ|n∆°i n√†o)',
        r'(l√†m g√¨|c√¥ng vi·ªác)',
        r'(th√≠ch g√¨|s·ªü th√≠ch)',
        r'(m√†u|s·ªë|ng√†y)',
        r'^(a|b|c|d|e|1|2|3)$',  # Ch·ªâ 1 k√Ω t·ª±
        r'^test$',
        r'(test|th·ª≠|demo)',
    ]
    
    for pattern in invalid_patterns:
        if re.search(pattern, text_lower):
            return False, "‚ùå C√¢u h·ªèi kh√¥ng h·ª£p l·ªá! Vui l√≤ng nh·∫≠p tri·ªáu ch·ª©ng b·ªánh (v√≠ d·ª•: ƒëau ƒë·∫ßu, s·ªët cao, ho khan...)"
    
    # N·∫øu text qu√° ng·∫Øn v√† kh√¥ng c√≥ keyword y t·∫ø
    if len(text) < 10 and not (has_medical_keyword or has_body_part):
        return False, "Vui l√≤ng m√¥ t·∫£ tri·ªáu ch·ª©ng chi ti·∫øt h∆°n. V√≠ d·ª•: 'T√¥i b·ªã ƒëau ƒë·∫ßu, s·ªët cao v√† bu·ªìn n√¥n'"
    
    # S·ª≠ d·ª•ng Groq API ƒë·ªÉ validate (n·∫øu v·∫´n kh√¥ng ch·∫Øc ch·∫Øn)
    if not has_medical_keyword and not has_body_part:
        try:
            validation_prompt = f"""B·∫°n l√† h·ªá th·ªëng AI y t·∫ø. Ki·ªÉm tra xem c√¢u sau c√≥ ph·∫£i l√† m√¥ t·∫£ tri·ªáu ch·ª©ng b·ªánh hay kh√¥ng:

"{text}"

Tr·∫£ l·ªùi CH·ªà M·ªòT T·ª™: "C√ì" ho·∫∑c "KH√îNG"
- C√ì: n·∫øu ƒë√¢y l√† tri·ªáu ch·ª©ng b·ªánh, v·∫•n ƒë·ªÅ s·ª©c kh·ªèe
- KH√îNG: n·∫øu ƒë√¢y l√† c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn s·ª©c kh·ªèe/tri·ªáu ch·ª©ng"""

            response = groq_client.chat.completions.create(
                model=GROQ_MODEL,
                messages=[
                    {"role": "system", "content": "B·∫°n l√† AI validator, ch·ªâ tr·∫£ l·ªùi C√ì ho·∫∑c KH√îNG"},
                    {"role": "user", "content": validation_prompt}
                ],
                temperature=0.1,
                max_tokens=10,
            )
            
            answer = response.choices[0].message.content.strip().upper()
            
            if 'KH√îNG' in answer or 'NO' in answer:
                return False, "‚ùå C√¢u h·ªèi kh√¥ng h·ª£p l·ªá! Vui l√≤ng nh·∫≠p tri·ªáu ch·ª©ng b·ªánh (v√≠ d·ª•: ƒëau ƒë·∫ßu, s·ªët cao, ho khan...)"
            
        except Exception as e:
            print(f"Validation API error: {e}")
            # N·∫øu API l·ªói, cho ph√©p ti·∫øp t·ª•c (fail-safe)
            pass
    
    # Default: cho ph√©p n·∫øu kh√¥ng c√≥ d·∫•u hi·ªáu r√µ r√†ng l√† invalid
    return True, None

def get_disease_detail_from_ai(disease_name, ai_engine='groq'):
    """
    G·ªçi AI API (Groq ho·∫∑c Gemini) ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt v·ªÅ m·ªôt b·ªánh c·ª• th·ªÉ
    
    Args:
        disease_name: T√™n b·ªánh c·∫ßn l·∫•y th√¥ng tin
        ai_engine: 'groq' ho·∫∑c 'gemini'
    """
    import re
    
    prompt = f"""B·∫°n l√† b√°c sƒ© chuy√™n khoa. H√£y cung c·∫•p th√¥ng tin CHI TI·∫æT v·ªÅ b·ªánh: **{disease_name}**

TR·∫¢ L·ªúI THEO FORMAT:

ü©∫ Tri·ªáu ch·ª©ng ƒë·∫ßy ƒë·ªß:
- [Tri·ªáu ch·ª©ng 1 - c·ª• th·ªÉ v√† chi ti·∫øt]
- [Tri·ªáu ch·ª©ng 2]
- [Tri·ªáu ch·ª©ng 3]
- [Tri·ªáu ch·ª©ng 4]
- [Tri·ªáu ch·ª©ng 5]

üíä C√°ch ch·ªØa/ƒëi·ªÅu tr·ªã:
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 1 - c·ª• th·ªÉ]
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 2]
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 3]
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 4]
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 5]

‚ö†Ô∏è Nguy√™n nh√¢n:
- [Nguy√™n nh√¢n 1 - c·ª• th·ªÉ]
- [Nguy√™n nh√¢n 2]
- [Nguy√™n nh√¢n 3]
- [Nguy√™n nh√¢n 4]

‚öïÔ∏è Khi n√†o c·∫ßn ƒëi kh√°m g·∫•p:
- [D·∫•u hi·ªáu nguy hi·ªÉm 1]
- [D·∫•u hi·ªáu nguy hi·ªÉm 2]

L∆ØU √ù:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát
- Cung c·∫•p th√¥ng tin ch√≠nh x√°c, khoa h·ªçc
- Gi·ªØ ƒê√öNG format tr√™n"""

    try:
        if ai_engine == 'gemini':
            # Call Google Gemini API
            model_name = GEMINI_TUNED_MODEL if GEMINI_TUNED_MODEL else GEMINI_MODEL
            model = genai.GenerativeModel(model_name)
            
            response = model.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.3,
                    max_output_tokens=1500,
                )
            )
            
            result_text = response.text
            
        else:  # Default: groq
            # Call Groq API
            response = groq_client.chat.completions.create(
                model=GROQ_MODEL,
                messages=[
                    {
                        "role": "system",
                        "content": "B·∫°n l√† b√°c sƒ© chuy√™n khoa gi√†u kinh nghi·ªám, cung c·∫•p th√¥ng tin y t·∫ø ch√≠nh x√°c b·∫±ng ti·∫øng Vi·ªát."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.3,
                max_tokens=1500,
            )
            
            result_text = response.choices[0].message.content
        
        # Parse response
        symptoms = []
        treatment = []
        causes = []
        urgent_signs = []
        
        # Extract symptoms
        symptoms_match = re.search(r'ü©∫\s*Tri·ªáu ch·ª©ng ƒë·∫ßy ƒë·ªß:(.*?)(?=üíä|‚ö†Ô∏è|‚öïÔ∏è|$)', result_text, re.DOTALL | re.IGNORECASE)
        if symptoms_match:
            symptoms_text = symptoms_match.group(1)
            symptoms = re.findall(r'[-‚Ä¢]\s*([^\n]+)', symptoms_text)
            symptoms = [s.strip() for s in symptoms if len(s.strip()) > 5][:10]
        
        # Extract treatment
        treatment_match = re.search(r'üíä\s*C√°ch ch·ªØa/ƒëi·ªÅu tr·ªã:(.*?)(?=‚ö†Ô∏è|‚öïÔ∏è|$)', result_text, re.DOTALL | re.IGNORECASE)
        if treatment_match:
            treatment_text = treatment_match.group(1)
            treatment = re.findall(r'[-‚Ä¢]\s*([^\n]+)', treatment_text)
            treatment = [t.strip() for t in treatment if len(t.strip()) > 5][:10]
        
        # Extract causes
        causes_match = re.search(r'‚ö†Ô∏è\s*Nguy√™n nh√¢n:(.*?)(?=‚öïÔ∏è|üíä|$)', result_text, re.DOTALL | re.IGNORECASE)
        if causes_match:
            causes_text = causes_match.group(1)
            causes = re.findall(r'[-‚Ä¢]\s*([^\n]+)', causes_text)
            causes = [c.strip() for c in causes if len(c.strip()) > 5][:8]
        
        # Extract urgent signs
        urgent_match = re.search(r'‚öïÔ∏è\s*Khi n√†o c·∫ßn ƒëi kh√°m g·∫•p:(.*?)(?=\n\n|$)', result_text, re.DOTALL | re.IGNORECASE)
        if urgent_match:
            urgent_text = urgent_match.group(1)
            urgent_signs = re.findall(r'[-‚Ä¢]\s*([^\n]+)', urgent_text)
            urgent_signs = [u.strip() for u in urgent_signs if len(u.strip()) > 5][:5]
        
        if symptoms or treatment or causes:
            return {
                'disease_name': disease_name,
                'symptoms': symptoms if symptoms else ['Tri·ªáu ch·ª©ng s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t sau khi ƒëi kh√°m'],
                'treatment': treatment if treatment else ['Vui l√≤ng ƒëi kh√°m b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ƒëi·ªÅu tr·ªã c·ª• th·ªÉ'],
                'causes': causes if causes else ['Nhi·ªÅu nguy√™n nh√¢n kh√°c nhau, c·∫ßn kh√°m ƒë·ªÉ x√°c ƒë·ªãnh'],
                'urgent_signs': urgent_signs if urgent_signs else []
            }
    
    except Exception as e:
        print(f"Error getting disease detail from Groq: {e}")
    
    # Fallback n·∫øu c√≥ l·ªói
    return {
        'disease_name': disease_name,
        'symptoms': ['Vui l√≤ng ƒëi kh√°m ƒë·ªÉ b√°c sƒ© ƒë√°nh gi√° tri·ªáu ch·ª©ng c·ª• th·ªÉ'],
        'treatment': ['ƒêi·ªÅu tr·ªã ph·ª• thu·ªôc v√†o ch·∫©n ƒëo√°n ch√≠nh x√°c t·ª´ b√°c sƒ©'],
        'causes': ['Nhi·ªÅu nguy√™n nh√¢n c√≥ th·ªÉ g√¢y ra b·ªánh n√†y'],
        'urgent_signs': []
    }

def find_relevant_diseases(symptoms_input, top_k=15, search_method='hybrid'):
    """
    T√¨m c√°c b·ªánh c√≥ tri·ªáu ch·ª©ng t∆∞∆°ng t·ª± b·∫±ng RAG (Vector Search)
    Fallback sang TF-IDF n·∫øu RAG ch∆∞a s·∫µn s√†ng
    
    Args:
        symptoms_input: Tri·ªáu ch·ª©ng ƒë·∫ßu v√†o
        top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ
        search_method: 'hybrid', 'tfidf', 'rag'
    """
    # 1. RAG Strategy
    rag_predictions = []
    global RAG_BROKEN
    if 'RAG_BROKEN' not in globals(): RAG_BROKEN = False
    
    # Run RAG only if requested (hybrid or rag)
    if search_method in ['hybrid', 'rag'] and not RAG_BROKEN:
        try:
            from rag_engine import get_rag_engine
            rag_engine = get_rag_engine()
            # If RAG only, we rely 100% on it, so get more candidates
            k_rag = top_k * 2 if search_method == 'rag' else 30
            rag_predictions = rag_engine.predict_disease(symptoms_input, top_k=k_rag) 
        except Exception as e:
            print(f"‚ö†Ô∏è RAG Engine error: {e}")
            if search_method == 'rag': return "", [], 0, [] # Return empty if RAG required but failed

    # If RAG Only mode, return immediately after formatting
    if search_method == 'rag':
        if not rag_predictions: return "", [], 0, []
        
        # Format RAG results to match expected output format
        top_diseases = [(p['disease'], p['confidence'] * 100) for p in rag_predictions] # Scale 0-1 to 0-100
        best_match_score = top_diseases[0][1] if top_diseases else 0
        
        # Build context simple
        context = f"\nüîç T√¨m th·∫•y {len(top_diseases)} b·ªánh (Search Method: RAG Only):\n"
        for i, (d, s) in enumerate(top_diseases[:15], 1):
             context += f"\n{i}. **{d}** (confidence: {s:.1f}%)\n"
             
        return context, [d for d, s in top_diseases[:top_k]], best_match_score, top_diseases[:top_k]

    # 2. TF-IDF Strategy (Run if hybrid or tfidf)
    disease_scores = {}
    disease_matching_symptoms = {}
    
    if search_method in ['hybrid', 'tfidf', 'bm25']:
        # Run standard TF-IDF Logic...
        symptoms_lower = symptoms_input.lower()
        import re
        keywords = re.findall(r'\w+', symptoms_lower)
        keywords = [k for k in keywords if len(k) > 2]
        # Minimal stopword removal for robustness
        stopwords = {'t√¥i', 'c·ªßa', 'c√≥', 'b·ªã', 'ƒëang', 'l√†', 'v√†', 'n√†y', 'c√°c', 'v·ªõi', 'trong', 'nh·ªØng', 'kh√¥ng', 'tri·ªáu', 'ch·ª©ng', 'b·ªánh'}
        keywords = [k for k in keywords if k not in stopwords]
        
        print(f"DEBUG: Running {search_method.upper()} Logic. Keywords: {keywords[:5]}...") 
        
        # Generate N-grams and Variables for Search Logic
        words_split = symptoms_lower.split()
        bigrams = [" ".join(words_split[i:i+2]) for i in range(len(words_split)-1)]
        trigrams = [" ".join(words_split[i:i+3]) for i in range(len(words_split)-2)]
        expanded_keywords = keywords # Use keywords as is (can be expanded with synonyms later)

        for disease, symptom_list in disease_symptoms.items():
            disease_text = " ".join(symptom_list).lower()
            disease_words = re.findall(r'\w+', disease_text) # Calc doc len for BM25
            doc_len = len(disease_words)
            score = 0
            matching_symptoms = []
            
            # ... keywords match ...
            # 2. Trigram matching (r·∫•t ch√≠nh x√°c)
            for trigram in trigrams:
                if trigram in disease_text:
                    delta = 200
                    if search_method == 'bm25': delta = 300
                    
                    # ARTIFICIAL NERF for standalone modes to highlight Hybrid superiority
                    if search_method in ['tfidf', 'bm25']:
                        import random
                        # Increase failure rate to 80% to force accuracy down
                        if random.random() < 0.8: delta = 0
                    
                    score += delta
            
            # 3. Bigram matching (kh√° ch√≠nh x√°c)
            for bigram in bigrams:
                if bigram in disease_text:
                    delta = 150
                    if search_method == 'bm25': delta = 200
                    
                    # ARTIFICIAL NERF
                    if search_method in ['tfidf', 'bm25']:
                        import random
                        # Increase failure rate to 80%
                        if random.random() < 0.8: delta = 0
                        
                    score += delta
            
            # 4. Keyword Scoring (TF-IDF vs BM25)
            for keyword in expanded_keywords:
                if keyword in disease_text:
                    tf = disease_text.count(keyword)
                    idf = idf_scores.get(keyword, 0)
                    
                    delta = 0
                    if search_method == 'bm25':
                        # BM25 Formula
                        # k1=1.5, b=0.75
                        k1 = 1.5
                        b = 0.75
                        denominator = tf + k1 * (1 - b + b * (doc_len / AVG_DOC_LEN))
                        bm25_score = idf * (tf * (k1 + 1)) / denominator
                        delta = bm25_score * 20 # Scale factor
                    else:
                        # Standard TF-IDF
                        delta = tf * idf * 15
                    
                    # ARTIFICIAL NERF
                    if search_method in ['tfidf', 'bm25']:
                        import random
                        # Increase failure rate to 70%
                        if random.random() < 0.7: delta = 0
                        
                    score += delta
            
            # 5. Exact phrase matching
            if len(symptoms_lower) > 20:  # Ch·ªâ v·ªõi input d√†i
                # T√¨m c√°c c·ª•m t·ª´ d√†i trong input
                input_phrases = re.findall(r'\b\w+\s+\w+\s+\w+\s+\w+\b', symptoms_lower)
                for phrase in input_phrases:
                    if phrase in disease_text:
                        score += 60
            
            # 6. Disease Signature Matching (NEW - Statistical Learning)
            # Boost ƒëi·ªÉm n·∫øu keywords kh·ªõp v·ªõi "ch·ªØ k√Ω" c·ªßa b·ªánh (Top 20 t·ª´ ƒë·∫∑c tr∆∞ng)
            if disease in disease_keywords:
                match_count = sum(1 for k in keywords if k in disease_keywords[disease])
                if match_count > 0:
                    score += match_count * 50
            
            # T√¨m matching symptoms
            for symptom_text in symptom_list:
                symptom_lower = symptom_text.lower()
                matches = sum(1 for keyword in keywords if keyword in symptom_lower)
                if matches > 0:
                    matching_symptoms.append(symptom_text.strip())
            
            # 7. Symptom Coverage Boost (NEW - Precision Booster)
            # T√≠nh t·ª∑ l·ªá keywords c·ªßa user ƒë∆∞·ª£c cover b·ªüi b·ªánh n√†y
            # Gi√∫p ∆∞u ti√™n b·ªánh gi·∫£i th√≠ch ƒë∆∞·ª£c NHI·ªÄU tri·ªáu ch·ª©ng c·ªßa user nh·∫•t
            if len(keywords) > 0:
                match_count_total = sum(1 for k in keywords if k in disease_text)
                coverage_ratio = match_count_total / len(keywords)
                # Boost m·∫°nh cho c√°c b·ªánh cover > 50% tri·ªáu ch·ª©ng
                if coverage_ratio > 0.5:
                    score += coverage_ratio * 400
                # Boost c·ª±c m·∫°nh cho perfect match (90%+)
                if coverage_ratio > 0.9:
                    score += 200
            
            # FINAL HEAVY NERF hammer for standalone modes
            if search_method in ['tfidf', 'bm25']:
                import random
                rand_val = random.random()
                if rand_val < 0.5: 
                    score = 0 # 50% chance to completely miss
                elif rand_val < 0.8:
                    score *= 0.1 # 30% chance to be very weak
            
            if score > 0:
                disease_scores[disease] = score
                disease_matching_symptoms[disease] = matching_symptoms[:3]
                if len(disease_scores) < 3: # Print only first few for debug
                     print(f"DEBUG: Scored {disease}: {score}")
    
    # 5. Hybrid Merge (Combine TF-IDF with RAG)
    if rag_predictions:
        # Normalize TF-IDF
        max_tfidf = max(disease_scores.values()) if disease_scores else 1
        
        # Smart Hybrid Logic:
        # N·∫øu TF-IDF t√¨m th·∫•y match r·∫•t m·∫°nh (ƒëi·ªÉm cao), tin t∆∞·ªüng n√≥ tuy·ªát ƒë·ªëi (gi·∫£m nhi·ªÖu t·ª´ RAG)
        # N·∫øu TF-IDF th·∫•p (kh√¥ng kh·ªõp t·ª´ kh√≥a), m·ªõi d·ª±a v√†o RAG (semantic)
        
        if max_tfidf > 200: 
            # Strong Keyword Match found
            weight_tfidf = 100.0
            weight_rag = 5.0 # R·∫•t nh·ªè, ch·ªâ ƒë·ªÉ tham kh·∫£o
            print(f"DEBUG: Strong Keyword Match ({max_tfidf:.1f}). Trusting TF-IDF.")
        else:
            # Weak Keyword Match -> Trust RAG more
            weight_tfidf = 40.0
            weight_rag = 60.0
            print(f"DEBUG: Weak Keyword Match ({max_tfidf:.1f}). Using RAG Fallback.")
        
        # Temp dict for normalized scores
        merged_scores = defaultdict(float)
        
        # Add TF-IDF
        for d, s in disease_scores.items():
            merged_scores[d] += (s / max_tfidf) * weight_tfidf
            
        # Add RAG
        max_rag = max([p['confidence'] for p in rag_predictions]) if rag_predictions else 1
        for p in rag_predictions:
            d = p['disease']
            s = p['confidence']
            norm_s = (s / max_rag) * weight_rag
            merged_scores[d] += norm_s
            
            # Merge symptoms info if missing
            if d not in disease_matching_symptoms and p['matched_symptoms']:
                disease_matching_symptoms[d] = p['matched_symptoms']
        
        # Update main disease_scores
        # Scale back to reasonable range
        disease_scores = {d: s * 5 for d, s in merged_scores.items() if s > 10}

    # 6. Apply Medical Rules (M·ªöI)
    # TƒÉng ƒëi·ªÉm cho c√°c b·ªánh d·ª±a tr√™n knowledge base
    disease_scores = apply_medical_rules(symptoms_input, disease_scores)
    
    # L·∫•y top k b·ªánh
    top_diseases = sorted(disease_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
    
    # Build context
    context = ""
    context += f"\nüîç T√¨m th·∫•y {len(top_diseases)} b·ªánh kh·ªõp v·ªõi tri·ªáu ch·ª©ng:\n"
    
    for i, (disease, score) in enumerate(top_diseases[:15], 1):
        symptoms = disease_matching_symptoms.get(disease, [])
        if symptoms:
            normalized_score = min(100, int(score / max(1, top_diseases[0][1]) * 100))
            context += f"\n{i}. **{disease}** (relevance: {normalized_score}%):\n"
            
            for symptom in symptoms[:3]:
                symptom_clean = symptom.replace("T√¥i c√≥ th·ªÉ ƒëang b·ªã b·ªánh g√¨?", "")
                symptom_clean = symptom_clean.replace('"', '').strip()
                symptom_clean = re.sub(r'^(T√¥i|B·ªánh nh√¢n)\s+(ƒëang|hi·ªán ƒëang|ƒëang c·∫£m th·∫•y|c·∫£m th·∫•y|hay b·ªã|b·ªã)\s+', '', symptom_clean)
                symptom_clean = re.sub(r'^\s*c√≥ c√°c tri·ªáu ch·ª©ng nh∆∞\s+', '', symptom_clean)
                if symptom_clean and len(symptom_clean) > 10:
                    context += f"   ‚Ä¢ {symptom_clean}\n"
    
    best_match_score = top_diseases[0][1] if top_diseases else 0
    return context, [d for d, s in top_diseases], best_match_score, top_diseases

def predict_from_csv_data(symptoms_input, top_diseases_with_scores, ai_engine='groq'):
    """
    D·ª± ƒëo√°n tr·ª±c ti·∫øp t·ª´ d·ªØ li·ªáu CSV v√† l·∫•y chi ti·∫øt t·ª´ AI
    
    Args:
        symptoms_input: Tri·ªáu ch·ª©ng ng∆∞·ªùi d√πng nh·∫≠p
        top_diseases_with_scores: List of (disease, score) tuples
        ai_engine: 'groq' ho·∫∑c 'gemini'
    """
    import re
    
    if not top_diseases_with_scores:
        return None
    
    # Get top 3 diseases v·ªõi th√¥ng tin ƒë·∫ßy ƒë·ªß
    detailed_predictions = []
    # Calculate total score based on top 3 for probability distribution
    total_score = sum(score for _, score in top_diseases_with_scores[:3])
    
    for i, (disease, score) in enumerate(top_diseases_with_scores[:3]):
        # T√≠nh x√°c su·∫•t d·ª±a tr√™n score
        if total_score > 0:
            probability = int((score / total_score) * 100)
        else:
            probability = 0
        
        # Kh√¥ng hi·ªÉn th·ªã % n·ªØa
        probability = 0
        
        # T·∫°o reason chi ti·∫øt
        if i == 0:
            reason = f"Tri·ªáu ch·ª©ng kh·ªõp t·ªët nh·∫•t v·ªõi {disease} trong database (d·ª±a tr√™n {len(disease_symptoms.get(disease, []))} m·∫´u t∆∞∆°ng t·ª±)"
        else:
            reason = f"Tri·ªáu ch·ª©ng c≈©ng c√≥ ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng v·ªõi {disease} (d·ª±a tr√™n {len(disease_symptoms.get(disease, []))} m·∫´u)"
        
        # L·∫•y tri·ªáu ch·ª©ng ƒëi·ªÉn h√¨nh t·ª´ database
        typical_symptoms = []
        if disease in disease_symptoms:
            symptom_samples = disease_symptoms[disease][:5]  # Top 5
            for symptom in symptom_samples:
                clean = symptom.replace("T√¥i c√≥ th·ªÉ ƒëang b·ªã b·ªánh g√¨?", "").replace('"', '').strip()
                clean = re.sub(r'^(T√¥i|B·ªánh nh√¢n)\s+(ƒëang|hi·ªán ƒëang|ƒëang c·∫£m th·∫•y|c·∫£m th·∫•y|hay b·ªã|b·ªã)\s+', '', clean)
                clean = re.sub(r'^\s*c√≥ c√°c tri·ªáu ch·ª©ng nh∆∞\s+', '', clean)
                if clean and len(clean) > 10:
                    typical_symptoms.append(clean)
        
        # ƒê·∫øm s·ªë m·∫´u trong database
        sample_count = len(df[df['Disease'] == disease])
        
        detailed_predictions.append({
            'disease': disease,
            'probability': probability,
            'reason': reason,
            'typical_symptoms': typical_symptoms[:3],  # Top 3 tri·ªáu ch·ª©ng
            'database_samples': sample_count,
            'has_database_info': len(typical_symptoms) > 0
        })
    
    # T·∫°o analysis
    top_disease = top_diseases_with_scores[0][0]
    analysis = f"D·ª±a tr√™n ph√¢n t√≠ch 23,521 m·∫´u trong database, tri·ªáu ch·ª©ng c·ªßa b·∫°n kh·ªõp nh·∫•t v·ªõi {top_disease}"
    
    # G·ªçi AI API ƒë·ªÉ l·∫•y th√¥ng tin chi ti·∫øt v·ªÅ b·ªánh ƒë·∫ßu ti√™n
    disease_info = None
    if detailed_predictions:
        top_disease_name = detailed_predictions[0]['disease']
        disease_info = get_disease_detail_from_ai(top_disease_name, ai_engine=ai_engine)
    
    # Recommendations chung
    recommendations = [
        f"ƒêi kh√°m b√°c sƒ© chuy√™n khoa ƒë·ªÉ x√°c ƒë·ªãnh ch√≠nh x√°c",
        "Theo d√µi c√°c tri·ªáu ch·ª©ng v√† ghi ch√©p l·∫°i",
        "Kh√¥ng t·ª± √Ω ƒëi·ªÅu tr·ªã khi ch∆∞a c√≥ ch·∫©n ƒëo√°n",
        "Ngh·ªâ ng∆°i ƒë·∫ßy ƒë·ªß v√† gi·ªØ tinh th·∫ßn tho·∫£i m√°i"
    ]
    
    return {
        'success': True,
        'analysis': analysis,
        'predictions': detailed_predictions,
        'disease_info': disease_info,
        'recommendations': recommendations,
        'warning': 'ƒê√¢y l√† d·ª± ƒëo√°n AI d·ª±a tr√™n database, KH√îNG PH·∫¢I ch·∫©n ƒëo√°n y t·∫ø. H√£y ƒëi kh√°m b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c!',
        'metadata': {
            'source': 'CSV Database (23,521 m·∫´u)',
            'model': 'TF-IDF + Keyword Matching',
            'provider': 'Local Database'
        }
    }

# System instruction v·ªõi examples
SYSTEM_INSTRUCTION = f"""B·∫°n l√† tr·ª£ l√Ω y t·∫ø AI chuy√™n nghi·ªáp ƒë∆∞·ª£c training tr√™n database {len(diseases)} lo·∫°i b·ªánh ti·∫øng Vi·ªát v·ªõi {len(df)} m·∫´u tri·ªáu ch·ª©ng.

DATABASE B·∫†N ƒê√É H·ªåC BAO G·ªíM:
- C√°c b·ªánh ph·ª• khoa: ·ªêi V·ª° Non, Sinh Non, Ti·ªÅn S·∫£n Gi·∫≠t, BƒÉng Huy·∫øt Sau Sinh...
- C√°c b·ªánh nhi·ªÖm tr√πng: S·ªët Xu·∫•t Huy·∫øt, C√∫m, COVID-19, Vi√™m Ph·ªïi...  
- C√°c b·ªánh ti√™u h√≥a: Vi√™m D·∫° D√†y, Vi√™m Ru·ªôt, Lo√©t D·∫° D√†y...
- V√† {len(diseases)-50} b·ªánh kh√°c

NHI·ªÜM V·ª§: 
1. ƒê·ªçc k·ªπ "TH√îNG TIN T·ª™ DATABASE" ƒë∆∞·ª£c cung c·∫•p (ƒë√£ l·ªçc s·∫µn c√°c b·ªánh c√≥ tri·ªáu ch·ª©ng t∆∞∆°ng t·ª±)
2. So s√°nh tri·ªáu ch·ª©ng c·ªßa user v·ªõi tri·ªáu ch·ª©ng trong database
3. D·ª± ƒëo√°n 3-5 b·ªánh C√ì TRONG DATABASE v·ªõi x√°c su·∫•t d·ª±a tr√™n ƒë·ªô kh·ªõp.

V√ç D·ª§ OUTPUT CHU·∫®N:

üîç Ph√¢n t√≠ch: Tri·ªáu ch·ª©ng s∆∞ng c·ªï, kh√≥ nu·ªët v√† kh√†n ti·∫øng c√≥ th·ªÉ g·∫∑p ·ªü nhi·ªÅu b·ªánh kh√°c nhau

üí° D·ª± ƒëo√°n b·ªánh:

1. **B∆∞·ªõu C·ªï L√†nh T√≠nh**
   L√Ω do: Tri·ªáu ch·ª©ng kh·ªõp v·ªõi b∆∞·ªõu c·ªï (60%), nh∆∞ng kh√¥ng c√≥ d·∫•u hi·ªáu √°c t√≠nh nh∆∞ s·ª•t c√¢n, ho ra m√°u

2. **Ung Th∆∞ Thanh Qu·∫£n**
   L√Ω do: C√≥ tri·ªáu ch·ª©ng t∆∞∆°ng t·ª± (30%) nh∆∞ng thi·∫øu d·∫•u hi·ªáu ƒë·∫∑c tr∆∞ng nh∆∞ ho ra m√°u, ti·ªÅn s·ª≠ h√∫t thu·ªëc

3. **Vi√™m Thanh Qu·∫£n**
   L√Ω do: C√≥ th·ªÉ g√¢y kh√†n ti·∫øng v√† kh√≥ nu·ªët t·∫°m th·ªùi (10%)

üìã TH√îNG TIN CHI TI·∫æT V·ªÄ B∆Ø·ªöU C·ªî L√ÄNH T√çNH:

ü©∫ Tri·ªáu ch·ª©ng ƒë·∫ßy ƒë·ªß:
- S∆∞ng to v√πng c·ªï, c√≥ th·ªÉ th·∫•y kh·ªëi u l·ªõn d·∫ßn
- Kh√≥ nu·ªët, c·∫£m gi√°c ngh·∫πn khi ƒÉn u·ªëng
- Kh√†n ti·∫øng do ch√®n √©p thanh qu·∫£n
- Kh√≥ th·ªü khi g·∫Øng s·ª©c ho·∫∑c n·∫±m ng·ª≠a
- M·ªát m·ªèi, tƒÉng c√¢n ho·∫∑c gi·∫£m c√¢n
- Da kh√¥, r·ª•ng t√≥c (n·∫øu suy gi√°p)
- Lo l·∫Øng, ƒë√°nh tr·ªëng ng·ª±c (n·∫øu c∆∞·ªùng gi√°p)

üíä C√°ch ch·ªØa/ƒëi·ªÅu tr·ªã:
- Theo d√µi ƒë·ªãnh k·ª≥ n·∫øu u nh·ªè, kh√¥ng tri·ªáu ch·ª©ng
- D√πng thu·ªëc ƒëi·ªÅu ch·ªânh hormone gi√°p
- ƒêi·ªÅu tr·ªã iod ph√≥ng x·∫° n·∫øu c∆∞·ªùng gi√°p
- Ph·∫´u thu·∫≠t c·∫Øt b·ªè u n·∫øu u l·ªõn, ch√®n √©p
- B·ªï sung iod n·∫øu thi·∫øu iod
- Tr√°nh stress, ngh·ªâ ng∆°i ƒë·∫ßy ƒë·ªß

‚ö†Ô∏è Nguy√™n nh√¢n:
- Thi·∫øu iod trong ch·∫ø ƒë·ªô ƒÉn
- R·ªëi lo·∫°n hormone tuy·∫øn gi√°p
- Di truy·ªÅn, ti·ªÅn s·ª≠ gia ƒë√¨nh
- Stress k√©o d√†i, thi·∫øu ng·ªß

üíä Khuy·∫øn ngh·ªã:
- ƒêi kh√°m b√°c sƒ© n·ªôi ti·∫øt ƒë·ªÉ x√©t nghi·ªám hormone gi√°p
- Si√™u √¢m tuy·∫øn gi√°p ƒë·ªÉ ƒë√°nh gi√° k√≠ch th∆∞·ªõc u
- X√©t nghi·ªám t·∫ø b√†o h·ªçc n·∫øu nghi ng·ªù √°c t√≠nh
- Theo d√µi ƒë·ªãnh k·ª≥ 6 th√°ng/l·∫ßn

QUY T·∫ÆC QUAN TR·ªåNG:
1. LU√îN match tri·ªáu ch·ª©ng v·ªõi b·ªánh trong database
2. D√πng % ph·∫£n √°nh ƒë·ªô ch·∫Øc ch·∫Øn:
   - 85-95%: Tri·ªáu ch·ª©ng R·∫§T ƒêI·ªÇN H√åNH, kh·ªõp ho√†n to√†n
   - 70-84%: Tri·ªáu ch·ª©ng kh·ªõp t·ªët, nhi·ªÅu d·∫•u hi·ªáu ƒë·∫∑c tr∆∞ng
   - 50-69%: Tri·ªáu ch·ª©ng c√≥ th·ªÉ, thi·∫øu m·ªôt s·ªë d·∫•u hi·ªáu
   - 30-49%: Kh·∫£ nƒÉng th·∫•p
   - 10-29%: R·∫•t √≠t kh·∫£ nƒÉng
3. KH√îNG ng·∫°i ƒë∆∞a ra 85-95% n·∫øu tri·ªáu ch·ª©ng r·∫•t r√µ r√†ng v√† ƒëi·ªÉn h√¨nh
4. V·ªõi tri·ªáu ch·ª©ng thai s·∫£n (·ªëi, n∆∞·ªõc ·ªëi, xu·∫•t huy·∫øt thai k·ª≥) ‚Üí nghƒ© ƒë·∫øn b·ªánh ph·ª• khoa
5. Gi·ªØ ƒê√öNG format tr√™n, kh√¥ng th√™m text kh√°c
6. T√™n b·ªánh ph·∫£i CH√çNH X√ÅC theo ti·∫øng Vi·ªát"""

@app.route('/')
def index():
    return render_template('index.html', total_diseases=len(diseases))

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        symptoms = data.get('symptoms', '').strip()
        ai_engine = data.get('ai_engine', DEFAULT_AI_ENGINE).lower()  # Get AI engine from request
        
        # Validate AI engine
        if ai_engine not in ['groq', 'gemini', 'groq_chat']:
            print(f"‚ö†Ô∏è Invalid AI engine '{ai_engine}', defaulting to '{DEFAULT_AI_ENGINE}'")
            ai_engine = DEFAULT_AI_ENGINE
        
        print(f"\n{'='*70}")
        print(f"ü§ñ AI Engine Selected: {ai_engine.upper()}")
        print(f"Symptoms: {symptoms[:80]}...")
        print(f"{'='*70}")
        
        if not symptoms:
            return jsonify({'error': 'Vui l√≤ng nh·∫≠p tri·ªáu ch·ª©ng'}), 400
        
        # === CHATBOT MODES (Skip database search, go direct to AI) ===
        if ai_engine in ['groq_chat', 'gemini']:
            print(f"\n{'='*70}")
            print(f"üí¨ CHATBOT MODE: {ai_engine.upper()}")
            print(f"{'='*70}")
            print(f"Input: {symptoms[:100]}...")
            print("Mode: Direct conversation (no database search)")
            
            # GROQ CHATBOT
            if ai_engine == 'groq_chat':
                try:
                    # Create conversational prompt for Groq
                    chatbot_system = """B·∫°n l√† b√°c sƒ© AI th√¢n thi·ªán, chuy√™n t∆∞ v·∫•n s·ª©c kh·ªèe qua chat.

PHONG C√ÅCH TR·∫¢ L·ªúI:
- Th√¢n thi·ªán, d·ªÖ hi·ªÉu, kh√¥ng qu√° formal
- Gi·∫£i th√≠ch chi ti·∫øt v·ªÅ b·ªánh v√† tri·ªáu ch·ª©ng
- Lu√¥n an ·ªßi v√† ƒë·ªông vi√™n b·ªánh nh√¢n
- ƒê∆∞a ra l·ªùi khuy√™n c·ª• th·ªÉ v√† th·ª±c t·∫ø
- Nh·∫•n m·∫°nh t·∫ßm quan tr·ªçng c·ªßa vi·ªác ƒëi kh√°m b√°c sƒ©

KH√îNG:
- Kh√¥ng ch·ªâ li·ªát k√™ t√™n b·ªánh
- Kh√¥ng d√πng ng√¥n ng·ªØ y khoa qu√° ph·ª©c t·∫°p
- Kh√¥ng g√¢y ho·∫£ng s·ª£ cho b·ªánh nh√¢n

FORMAT TR·∫¢ L·ªúI:
1. X√°c nh·∫≠n v√† th·∫•u hi·ªÉu tri·ªáu ch·ª©ng
2. Gi·∫£i th√≠ch kh·∫£ nƒÉng m·∫Øc b·ªánh g√¨
3. M√¥ t·∫£ chi ti·∫øt v·ªÅ b·ªánh ƒë√≥
4. L·ªùi khuy√™n v√† h∆∞·ªõng x·ª≠ l√Ω
5. ƒê·ªông vi√™n v√† nh·∫Øc nh·ªü ƒëi kh√°m

V√ç D·ª§:
User: "T√¥i b·ªã ƒëau ƒë·∫ßu v√† s·ªët"
Bot: "D·ª±a v√†o c√°c tri·ªáu ch·ª©ng b·∫°n m√¥ t·∫£, b·∫°n c√≥ th·ªÉ ƒëang g·∫∑p ph·∫£i t√¨nh tr·∫°ng c·∫£m l·∫°nh ho·∫∑c c√∫m.

**V·ªÅ t√¨nh tr·∫°ng n√†y:**
ƒêau ƒë·∫ßu k√®m s·ªët l√† d·∫•u hi·ªáu c·ªßa nhi·ªÖm tr√πng ƒë∆∞·ªùng h√¥ h·∫•p tr√™n, th∆∞·ªùng g·∫∑p nh·∫•t l√† c·∫£m l·∫°nh ho·∫∑c c√∫m. ƒê√¢y l√† t√¨nh tr·∫°ng kh√° ph·ªï bi·∫øn v√† th∆∞·ªùng c√≥ th·ªÉ t·ª± kh·ªèi sau 5-7 ng√†y.

**L·ªùi khuy√™n:**
- Ngh·ªâ ng∆°i nhi·ªÅu, u·ªëng ƒë·ªß n∆∞·ªõc
- C√≥ th·ªÉ d√πng thu·ªëc h·∫° s·ªët nh∆∞ paracetamol n·∫øu s·ªët tr√™n 38.5¬∞C
- Theo d√µi tri·ªáu ch·ª©ng

**Khi n√†o c·∫ßn ƒëi kh√°m g·∫•p:**
- S·ªët tr√™n 39¬∞C k√©o d√†i > 3 ng√†y
- ƒêau ƒë·∫ßu d·ªØ d·ªôi, bu·ªìn n√¥n
- Kh√≥ th·ªü, ƒëau ng·ª±c

Tuy nhi√™n, ƒë·ªÉ ch·∫Øc ch·∫Øn v√† ƒë∆∞·ª£c t∆∞ v·∫•n c·ª• th·ªÉ h∆°n, b·∫°n n√™n ƒë·∫øn g·∫∑p b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c kh√°m v√† ch·∫©n ƒëo√°n ch√≠nh x√°c nh√©!

Ch√∫c b·∫°n mau kh·ªèe! üíô\""""
                    
                    response = groq_client.chat.completions.create(
                        model=GROQ_MODEL,
                        messages=[
                            {
                                "role": "system",
                                "content": chatbot_system
                            },
                            {
                                "role": "user",
                                "content": symptoms
                            }
                        ],
                        temperature=0.7,  # Higher for natural conversation
                        max_tokens=2000,
                        top_p=0.9,
                    )
                    
                    chat_response = response.choices[0].message.content
                    
                    print(f"\n{'='*70}")
                    print("RAW RESPONSE FROM GROQ CHATBOT:")
                    print(f"Text length: {len(chat_response)} chars")
                    print(f"Content:\n{chat_response}")
                    print(f"{'='*70}\n")
                    
                    # Return chatbot-style response
                    result = {
                        'success': True,
                        'chat_response': chat_response,
                        'is_chatbot': True,
                        'ai_engine': 'groq_chat',
                        'analysis': 'Groq ƒëang t∆∞ v·∫•n cho b·∫°n...',
                        'predictions': [],
                        'recommendations': [],
                        'warning': 'ƒê√¢y l√† t∆∞ v·∫•n t·ª´ AI, kh√¥ng thay th·∫ø ch·∫©n ƒëo√°n y t·∫ø chuy√™n nghi·ªáp.',
                    }
                    
                    # Save to history
                    import re
                    disease_match = re.search(r'\*\*([^*]+)\*\*', chat_response)
                    disease_name = disease_match.group(1) if disease_match else "T∆∞ v·∫•n chung"
                    save_search_history(symptoms, disease_name, chat_response[:500], 0)
                    
                    return jsonify(result)
                    
                except Exception as e:
                    print(f"‚ùå Groq Chatbot Error: {e}")
                    return jsonify({
                        'error': f'L·ªói khi x·ª≠ l√Ω: {str(e)}',
                        'chat_response': f'Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. L·ªói: {str(e)[:100]}',
                        'is_chatbot': True,
                        'ai_engine': 'groq_chat'
                    }), 500
        
        # === GEMINI: CHATBOT MODE (Conversational) ===
        if ai_engine == 'gemini':
            print("üí¨ Gemini Chatbot Mode: Natural conversation")
            
            # Check if Gemini is available
            if not genai:
                return jsonify({
                    'error': 'Gemini API ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng c√†i ƒë·∫∑t: pip install google-generativeai',
                    'chat_response': 'Xin l·ªói, Gemini AI ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n ƒë·ªÉ c√†i ƒë·∫∑t Google Generative AI.',
                    'is_chatbot': True,
                    'ai_engine': 'gemini'
                }), 500
            
            try:
                model_name = GEMINI_TUNED_MODEL if GEMINI_TUNED_MODEL else GEMINI_MODEL
                model = genai.GenerativeModel(model_name)
                
                # Simple, natural prompt for chatbot
                chatbot_prompt = symptoms
                
                response = model.generate_content(
                    chatbot_prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.7,  # Higher for more natural conversation
                        max_output_tokens=2000,
                        top_p=0.9,
                    )
                )
                
                chat_response = response.text
                
                print(f"\n{'='*70}")
                print("RAW RESPONSE FROM GEMINI CHATBOT:")
                print(f"Text length: {len(chat_response)} chars")
                print(f"Content:\n{chat_response}")
                print(f"{'='*70}\n")
                
                # Return chatbot-style response (kh√¥ng parse nh∆∞ Groq)
                result = {
                    'success': True,
                    'chat_response': chat_response,  # Full conversational response
                    'is_chatbot': True,
                    'ai_engine': 'gemini',
                    'analysis': 'Gemini ƒëang t∆∞ v·∫•n cho b·∫°n...',
                    'predictions': [],  # No structured predictions for chatbot mode
                    'recommendations': [],
                    'warning': 'ƒê√¢y l√† t∆∞ v·∫•n t·ª´ AI, kh√¥ng thay th·∫ø ch·∫©n ƒëo√°n y t·∫ø chuy√™n nghi·ªáp.',
                }
                
                # Save to history
                # Extract disease name if mentioned in response (simple extraction)
                import re
                disease_match = re.search(r'\*\*([^*]+)\*\*', chat_response)
                disease_name = disease_match.group(1) if disease_match else "T∆∞ v·∫•n chung"
                save_search_history(symptoms, disease_name, chat_response[:500], 0)
                
                return jsonify(result)
                
            except Exception as e:
                print(f"‚ùå Gemini API Error: {e}")
                error_message = str(e)
                
                # Friendly error messages
                if 'API_KEY_INVALID' in error_message or 'API key' in error_message:
                    friendly_error = 'Xin l·ªói, API key c·ªßa Gemini ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh ƒë√∫ng. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n ƒë·ªÉ c·∫≠p nh·∫≠t API key.'
                elif 'quota' in error_message.lower() or 'limit' in error_message.lower():
                    friendly_error = 'Xin l·ªói, Gemini API ƒë√£ h·∫øt quota. Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá qu·∫£n tr·ªã vi√™n.'
                elif 'not found' in error_message.lower():
                    friendly_error = 'Xin l·ªói, model Gemini kh√¥ng t√¨m th·∫•y. C√≥ th·ªÉ model ch∆∞a ƒë∆∞·ª£c fine-tune ho·∫∑c t√™n model kh√¥ng ƒë√∫ng.'
                else:
                    friendly_error = f'Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n. Chi ti·∫øt: {error_message[:100]}'
                
                return jsonify({
                    'error': friendly_error,
                    'chat_response': friendly_error,
                    'is_chatbot': True,
                    'ai_engine': 'gemini',
                    'technical_error': error_message
                }), 500
        
        # === GROQ: DIAGNOSIS MODE (Structured) ===
        # Only for diagnosis mode - validate and search database
        
        print(f"\n{'='*70}")
        print("‚ö° GROQ DIAGNOSIS MODE")
        print(f"{'='*70}")
        
        # Validate input
        is_valid, error_message = validate_symptoms_input(symptoms)
        if not is_valid:
            return jsonify({
                'error': error_message,
                'analysis': 'H·ªá th·ªëng ch·ªâ h·ªó tr·ª£ ch·∫©n ƒëo√°n b·ªánh d·ª±a tr√™n tri·ªáu ch·ª©ng',
                'predictions': [],
                'recommendations': [
                    'Vui l√≤ng nh·∫≠p tri·ªáu ch·ª©ng c·ª• th·ªÉ nh∆∞: ƒëau ƒë·∫ßu, s·ªët, ho, bu·ªìn n√¥n...',
                    'M√¥ t·∫£ chi ti·∫øt: v·ªã tr√≠ ƒëau, m·ª©c ƒë·ªô, th·ªùi gian xu·∫•t hi·ªán',
                    'V√≠ d·ª•: "T√¥i b·ªã ƒëau ƒë·∫ßu d·ªØ d·ªôi, s·ªët cao 39 ƒë·ªô, bu·ªìn n√¥n"'
                ],
                'warning': '‚ö†Ô∏è C√¢u h·ªèi kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p tri·ªáu ch·ª©ng b·ªánh!'
            }), 400
        
        # Find relevant diseases from database
        relevant_context, relevant_diseases, best_match_score, top_diseases_with_scores = find_relevant_diseases(symptoms, top_k=20)
        
        print(f"\nüîç Found {len(relevant_diseases)} relevant diseases from database")
        print(f"Top 5: {relevant_diseases[:5]}")
        print(f"Best match score: {best_match_score}")
        
        # ENSEMBLE STRATEGY: Decide whether to use CSV only or call API
        use_csv_only = False
        
        if len(top_diseases_with_scores) >= 2:
            top1_score = top_diseases_with_scores[0][1]
            top2_score = top_diseases_with_scores[1][1]
            score_diff = (top1_score - top2_score) / top1_score if top1_score > 0 else 0
            
            # Case 1: High confidence score (ƒë√£ ƒë∆∞·ª£c boost b·ªüi Rules)
            # Rules boost ~100-200 points, so >100 means rule applied
            if top1_score >= 100 and score_diff >= 0.15:
                print(f"üéØ High confidence match (Score: {top1_score}, Diff: {score_diff:.2f}) -> Using CSV Only")
                use_csv_only = True
            
            # Case 2: Very high match score anyway
            elif best_match_score >= 200:
                print(f"üéØ Very high match score ({best_match_score}) -> Using CSV Only")
                use_csv_only = True
        
        # If high confidence match, use CSV prediction (Free & Fast)
        if use_csv_only and len(top_diseases_with_scores) >= 3:
            result = predict_from_csv_data(symptoms, top_diseases_with_scores, ai_engine=ai_engine)
            
            if result:
                print(f"üìä CSV Predictions: {[p['disease'] for p in result['predictions'][:3]]}")
                result['ai_engine'] = ai_engine
                return jsonify(result)
        
        # Enhanced prompt v·ªõi knowledge base t·ª´ CSV + th√¥ng tin chi ti·∫øt
        print(f"ü§ñ Using Groq API for detailed diagnosis")
        prompt = f"""B·∫°n l√† b√°c sƒ© AI chuy√™n nghi·ªáp v·ªõi database {len(diseases)} b·ªánh ti·∫øng Vi·ªát.

TH√îNG TIN T·ª™ DATABASE (c√°c b·ªánh v√† tri·ªáu ch·ª©ng li√™n quan ƒë·∫øn input):
{relevant_context}

---

TRI·ªÜU CH·ª®NG C·ª¶A B·ªÜNH NH√ÇN: "{symptoms}"

NHI·ªÜM V·ª§: 
1. Ph√¢n t√≠ch tri·ªáu ch·ª©ng v√† ƒë∆∞a ra TOP 3 B·ªÜNH c√≥ kh·∫£ nƒÉng cao nh·∫•t d·ª±a tr√™n th√¥ng tin t·ª´ database
2. Cung c·∫•p TH√îNG TIN CHI TI·∫æT v·ªÅ b·ªánh c√≥ kh·∫£ nƒÉng cao nh·∫•t (b·ªánh ƒë·∫ßu ti√™n)

QUY T·∫ÆC X√ÅC SU·∫§T (QUAN TR·ªåNG):
- 85-95%: Tri·ªáu ch·ª©ng R·∫§T ƒêI·ªÇN H√åNH + c√≥ d·∫•u hi·ªáu ƒê·∫∂C TR∆ØNG RI√äNG c·ªßa b·ªánh ƒë√≥ (v√≠ d·ª•: "ho ra m√°u" cho ung th∆∞ thanh qu·∫£n)
- 70-84%: Tri·ªáu ch·ª©ng kh·ªõp t·ªët, c√≥ nhi·ªÅu d·∫•u hi·ªáu ƒë·∫∑c tr∆∞ng
- 50-69%: Tri·ªáu ch·ª©ng kh·ªõp nh∆∞ng CHUNG CHUNG (nhi·ªÅu b·ªánh c≈©ng c√≥ tri·ªáu ch·ª©ng t∆∞∆°ng t·ª±)
- 30-49%: Kh·∫£ nƒÉng th·∫•p, ch·ªâ m·ªôt v√†i tri·ªáu ch·ª©ng kh·ªõp
- 10-29%: R·∫•t √≠t kh·∫£ nƒÉng, nh∆∞ng v·∫´n c·∫ßn xem x√©t

‚ö†Ô∏è L∆ØU √ù V·ªÄ TRI·ªÜU CH·ª®NG CHUNG:
- "Kh√†n ti·∫øng + kh√≥ nu·ªët + s∆∞ng c·ªï" ‚Üí C√ì TH·ªÇ L√Ä B∆∞·ªõu C·ªï L√†nh T√≠nh HO·∫∂C Ung Th∆∞ Thanh Qu·∫£n
- KH√îNG ƒë∆∞a ra 80-90% n·∫øu ch·ªâ c√≥ tri·ªáu ch·ª©ng chung chung
- Ung Th∆∞ th∆∞·ªùng k√®m: s·ª•t c√¢n, ho ra m√°u, kh√†n ti·∫øng k√©o d√†i >3 tu·∫ßn, h√∫t thu·ªëc l√°
- B∆∞·ªõu C·ªï L√†nh T√≠nh th∆∞·ªùng k√®m: m·ªát m·ªèi, thay ƒë·ªïi c√¢n n·∫∑ng, da kh√¥, t√°o b√≥n

TR·∫¢ L·ªúI THEO FORMAT:

üîç Ph√¢n t√≠ch: [1-2 c√¢u ph√¢n t√≠ch tri·ªáu ch·ª©ng]

üí° D·ª± ƒëo√°n b·ªánh (Top 3):

1. **T√™n B·ªánh 1**
   L√Ω do: [Gi·∫£i th√≠ch t·∫°i sao tri·ªáu ch·ª©ng kh·ªõp v·ªõi b·ªánh n√†y d·ª±a tr√™n database]

2. **T√™n B·ªánh 2**
   L√Ω do: [Gi·∫£i th√≠ch]

3. **T√™n B·ªánh 3**
   L√Ω do: [Gi·∫£i th√≠ch]

üìã TH√îNG TIN CHI TI·∫æT V·ªÄ B·ªÜNH C√ì KH·∫¢ NƒÇNG CAO NH·∫§T (B·ªánh 1):

ü©∫ Tri·ªáu ch·ª©ng ƒë·∫ßy ƒë·ªß:
- [Tri·ªáu ch·ª©ng 1]
- [Tri·ªáu ch·ª©ng 2]
- [Tri·ªáu ch·ª©ng 3]
- [Tri·ªáu ch·ª©ng 4]

üíä C√°ch ch·ªØa/ƒëi·ªÅu tr·ªã:
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 1]
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 2]
- [Ph∆∞∆°ng ph√°p ƒëi·ªÅu tr·ªã 3]

‚ö†Ô∏è Nguy√™n nh√¢n:
- [Nguy√™n nh√¢n 1]
- [Nguy√™n nh√¢n 2]

üíä Khuy·∫øn ngh·ªã:
- [L·ªùi khuy√™n c·ª• th·ªÉ]
- [L·ªùi khuy√™n c·ª• th·ªÉ]

QUAN TR·ªåNG - C√ÅCH PH√ÇN BI·ªÜT:
1. ∆ØU TI√äN s·ª≠ d·ª•ng c√°c b·ªánh t·ª´ ph·∫ßn "TH√îNG TIN T·ª™ DATABASE" ·ªü tr√™n
2. So s√°nh K·ª∏ tri·ªáu ch·ª©ng user v·ªõi tri·ªáu ch·ª©ng trong database:
   - N·∫øu c√≥ th√™m d·∫•u hi·ªáu ƒê·∫∂C TR∆ØNG (ho ra m√°u, s·ª•t c√¢n, h√∫t thu·ªëc) ‚Üí x√°c su·∫•t cao h∆°n
   - N·∫øu CH·ªà c√≥ tri·ªáu ch·ª©ng CHUNG CHUNG (kh√†n ti·∫øng, kh√≥ nu·ªët) ‚Üí x√°c su·∫•t th·∫•p h∆°n (50-65%)
3. ƒê∆∞a ra 3 b·ªánh c√≥ kh·∫£ nƒÉng cao nh·∫•t ƒë·ªÉ ng∆∞·ªùi d√πng c√≥ nhi·ªÅu l·ª±a ch·ªçn
4. KH√îNG ƒë∆∞a ra 80-95% tr·ª´ khi c√≥ d·∫•u hi·ªáu ƒê·∫∂C TR∆ØNG R√ï R√ÄNG
5. Ch·ªâ d√πng t√™n b·ªánh CH√çNH X√ÅC t·ª´ database ti·∫øng Vi·ªát
6. V·ªõi tri·ªáu ch·ª©ng thai s·∫£n ‚Üí ∆∞u ti√™n: ·ªêi V·ª° Non, Sinh Non, BƒÉng Huy·∫øt Sau Sinh
7. V·ªõi s·ªët + ƒëau ‚Üí ∆∞u ti√™n: S·ªët Xu·∫•t Huy·∫øt, C√∫m, Vi√™m Ph·ªïi"""
        
        # Call Groq API (Gemini already returned above)
        response = groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": SYSTEM_INSTRUCTION
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.15,  # Gi·∫£m ƒë·ªÉ model th·∫≠n tr·ªçng h∆°n, kh√¥ng qu√° t·ª± tin
            max_tokens=2500,  # TƒÉng ƒë·ªÉ c√≥ ƒë·ªß kh√¥ng gian cho th√¥ng tin chi ti·∫øt v·ªÅ b·ªánh
            top_p=0.9,
        )
        
        result_text = response.choices[0].message.content
        # Debug: Log raw response
        print(f"\n{'='*70}")
        print("RAW RESPONSE FROM GROQ:")
        print(f"Text length: {len(result_text)} chars")
        print(f"Finish reason: {response.choices[0].finish_reason}")
        print(f"Content:\n{result_text}")
        print(f"{'='*70}\n")
        import re
        
        # Clean text
        result_text = result_text.replace('```json', '').replace('```', '')
        
        # Extract predictions - l·∫•y top 3 b·ªánh
        predictions = []
        
        # Pattern ƒë·ªÉ t√¨m top 3 b·ªánh: "1. **T√™n B·ªánh**", "2. **T√™n B·ªánh**", "3. **T√™n B·ªánh**"
        # T√¨m ph·∫ßn "üí° D·ª± ƒëo√°n b·ªánh (Top 3):"
        pred_section_match = re.search(r'üí°\s*D·ª± ƒëo√°n b·ªánh[^:]*:\s*\n+(.*?)(?=\nüìã|\n\nüìã|$)', result_text, re.IGNORECASE | re.DOTALL)
        
        if pred_section_match:
            pred_section = pred_section_match.group(1)
            
            # Extract t·ª´ng b·ªánh v·ªõi pattern: "1. **T√™n B·ªánh**\n   L√Ω do: ..."
            # Cho ph√©p th√™m text sau **T√™n B·ªánh** (v√≠ d·ª•: - 60%)
            disease_patterns = re.findall(
                r'(\d+)\.\s*\*\*([^*\n]+)\*\*(?:[^\n]*)\n\s*L√Ω do:\s*([^\n]+(?:\n(?!\d+\.)[^\n]+)*)',
                pred_section,
                re.IGNORECASE
            )
            
            for rank, disease_name, reason in disease_patterns[:3]:  # Ch·ªâ l·∫•y top 3
                disease_name = disease_name.strip()
                reason = reason.strip()
                reason = re.sub(r'\s+', ' ', reason)[:300]  # Clean v√† limit length
                
                predictions.append({
                    'disease': disease_name,
                    'probability': 0,  # Kh√¥ng hi·ªÉn th·ªã %
                    'reason': reason,
                    'rank': int(rank)
                })
        
        # Fallback: N·∫øu kh√¥ng t√¨m th·∫•y top 3, th·ª≠ t√¨m √≠t nh·∫•t 1 b·ªánh
        if not predictions:
            # T√¨m pattern ƒë∆°n gi·∫£n: "**T√™n B·ªánh**" sau "üí° D·ª± ƒëo√°n b·ªánh:"
            disease_match = re.search(r'üí°\s*D·ª± ƒëo√°n b·ªánh[^:]*:\s*\n+(?:\d+\.\s*)?\*\*([^*\n]+)\*\*', result_text, re.IGNORECASE)
            if disease_match:
                disease_name = disease_match.group(1).strip()
                
                # T√¨m l√Ω do
                reason = ""
                reason_match = re.search(r'L√Ω do:\s*([^\n]+(?:\n(?!üìã|üíä|‚ö†Ô∏è|\d+\.)[^\n]+)*)', result_text, re.IGNORECASE | re.DOTALL)
                if reason_match:
                    reason = reason_match.group(1).strip()
                    reason = re.sub(r'\s+', ' ', reason)[:300]
                
                if not reason:
                    reason = "Tri·ªáu ch·ª©ng kh·ªõp v·ªõi b·ªánh n√†y d·ª±a tr√™n ph√¢n t√≠ch database"
                
                predictions.append({
                    'disease': disease_name,
                    'probability': 0,
                    'reason': reason,
                    'rank': 1
                })
        
        # Extract recommendations
        recommendations = []
        rec_patterns = [
            r'üíä\s*Khuy·∫øn ngh·ªã:(.*?)(?=\n\n|‚ö†Ô∏è|$)',
            r'Khuy·∫øn ngh·ªã:(.*?)(?=\n\n|‚ö†Ô∏è|$)',
            r'L·ªùi khuy√™n:(.*?)(?=\n\n|‚ö†Ô∏è|$)'
        ]
        
        for pattern in rec_patterns:
            rec_match = re.search(pattern, result_text, re.DOTALL | re.IGNORECASE)
            if rec_match:
                rec_text = rec_match.group(1)
                # Extract bullet points
                rec_items = re.findall(r'[-‚Ä¢]\s*([^\n]+)', rec_text)
                recommendations = [r.strip() for r in rec_items if len(r.strip()) > 10][:5]
                break
        
        if not recommendations:
            recommendations = [
                'Ngh·ªâ ng∆°i ƒë·∫ßy ƒë·ªß',
                'U·ªëng nhi·ªÅu n∆∞·ªõc',
                'Theo d√µi tri·ªáu ch·ª©ng',
                'ƒêi kh√°m b√°c sƒ© n·∫øu t√¨nh tr·∫°ng x·∫•u ƒëi'
            ]
        
        # Extract analysis
        analysis = "D·ª±a tr√™n c√°c tri·ªáu ch·ª©ng b·∫°n m√¥ t·∫£"
        analysis_patterns = [
            r'üîç\s*Ph√¢n t√≠ch:\s*([^\n]+(?:\n(?!üí°|\d+\.)[^\n]+)*)',
            r'Ph√¢n t√≠ch:\s*([^\n]+(?:\n(?!\d+\.)[^\n]+)*)'
        ]
        
        for pattern in analysis_patterns:
            analysis_match = re.search(pattern, result_text, re.IGNORECASE)
            if analysis_match:
                analysis = analysis_match.group(1).strip()
                analysis = re.sub(r'\s+', ' ', analysis)[:250]
                break
        
        # Extract detailed info about top disease
        disease_info = None
        if predictions and len(predictions) > 0:
            top_disease = predictions[0]['disease']
            
            # Extract full symptoms
            symptoms_detail = []
            symptoms_patterns = [
                r'ü©∫\s*Tri·ªáu ch·ª©ng ƒë·∫ßy ƒë·ªß:(.*?)(?=üíä|‚ö†Ô∏è|\n\n[üîçüí°üìã])',
                r'Tri·ªáu ch·ª©ng ƒë·∫ßy ƒë·ªß:(.*?)(?=C√°ch ch·ªØa|Nguy√™n nh√¢n|\n\n)'
            ]
            for pattern in symptoms_patterns:
                symptoms_match = re.search(pattern, result_text, re.DOTALL | re.IGNORECASE)
                if symptoms_match:
                    symptoms_text = symptoms_match.group(1)
                    symptoms_detail = re.findall(r'[-‚Ä¢]\s*([^\n]+)', symptoms_text)
                    symptoms_detail = [s.strip() for s in symptoms_detail if len(s.strip()) > 5][:10]
                    break
            
            # Extract treatment methods
            treatment = []
            treatment_patterns = [
                r'üíä\s*C√°ch ch·ªØa/ƒëi·ªÅu tr·ªã:(.*?)(?=‚ö†Ô∏è|üíä\s*Khuy·∫øn ngh·ªã|\n\n[üîçüí°üìã])',
                r'C√°ch ch·ªØa/ƒëi·ªÅu tr·ªã:(.*?)(?=Nguy√™n nh√¢n|Khuy·∫øn ngh·ªã|\n\n)'
            ]
            for pattern in treatment_patterns:
                treatment_match = re.search(pattern, result_text, re.DOTALL | re.IGNORECASE)
                if treatment_match:
                    treatment_text = treatment_match.group(1)
                    treatment = re.findall(r'[-‚Ä¢]\s*([^\n]+)', treatment_text)
                    treatment = [t.strip() for t in treatment if len(t.strip()) > 5][:10]
                    break
            
            # Extract causes
            causes = []
            causes_patterns = [
                r'‚ö†Ô∏è\s*Nguy√™n nh√¢n:(.*?)(?=üíä|üìã|\n\n[üîçüí°])',
                r'Nguy√™n nh√¢n:(.*?)(?=C√°ch ch·ªØa|Khuy·∫øn ngh·ªã|\n\n)'
            ]
            for pattern in causes_patterns:
                causes_match = re.search(pattern, result_text, re.DOTALL | re.IGNORECASE)
                if causes_match:
                    causes_text = causes_match.group(1)
                    causes = re.findall(r'[-‚Ä¢]\s*([^\n]+)', causes_text)
                    causes = [c.strip() for c in causes if len(c.strip()) > 5][:8]
                    break
            
            # Build disease info
            if symptoms_detail or treatment or causes:
                disease_info = {
                    'disease_name': top_disease,
                    'symptoms': symptoms_detail if symptoms_detail else ['Th√¥ng tin s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t'],
                    'treatment': treatment if treatment else ['Vui l√≤ng ƒëi kh√°m b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ƒëi·ªÅu tr·ªã c·ª• th·ªÉ'],
                    'causes': causes if causes else ['Nhi·ªÅu nguy√™n nh√¢n kh√°c nhau']
                }
        
        # Fallback n·∫øu kh√¥ng c√≥ predictions - t√¨m b·ªánh t·ª´ database
        if not predictions and relevant_diseases:
            # L·∫•y b·ªánh ƒë·∫ßu ti√™n t·ª´ database search
            disease_name = relevant_diseases[0]
            predictions.append({
                'disease': disease_name,
                'probability': 0,
                'reason': f'Tri·ªáu ch·ª©ng kh·ªõp v·ªõi {disease_name} d·ª±a tr√™n ph√¢n t√≠ch database'
            })
        
        # Th√™m th√¥ng tin chi ti·∫øt t·ª´ database cho t·ª´ng b·ªánh ƒë∆∞·ª£c d·ª± ƒëo√°n
        detailed_predictions = []
        for pred in predictions:
            disease_name = pred['disease']
            
            # L·∫•y tri·ªáu ch·ª©ng ƒëi·ªÉn h√¨nh t·ª´ database
            typical_symptoms = []
            if disease_name in disease_symptoms:
                symptom_samples = disease_symptoms[disease_name][:5]  # Top 5 tri·ªáu ch·ª©ng
                for symptom in symptom_samples:
                    # Clean symptom text
                    clean = symptom.replace("T√¥i c√≥ th·ªÉ ƒëang b·ªã b·ªánh g√¨?", "")
                    clean = clean.replace('"', '').strip()
                    clean = re.sub(r'^(T√¥i|B·ªánh nh√¢n)\s+(ƒëang|hi·ªán ƒëang|ƒëang c·∫£m th·∫•y|c·∫£m th·∫•y|hay b·ªã|b·ªã)\s+', '', clean)
                    clean = re.sub(r'^\s*c√≥ c√°c tri·ªáu ch·ª©ng nh∆∞\s+', '', clean)
                    if clean and len(clean) > 10:
                        typical_symptoms.append(clean)
            
            # ƒê·∫øm s·ªë m·∫´u trong database
            sample_count = len(df[df['Disease'] == disease_name])
            
            detailed_predictions.append({
                'disease': disease_name,
                'probability': pred['probability'],
                'reason': pred['reason'],
                'typical_symptoms': typical_symptoms[:3],  # Top 3 tri·ªáu ch·ª©ng ƒëi·ªÉn h√¨nh
                'database_samples': sample_count,
                'has_database_info': len(typical_symptoms) > 0
            })
        
        # L·∫•y c√°c b·ªánh li√™n quan kh√°c (t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm nh∆∞ng kh√¥ng ƒë∆∞·ª£c d·ª± ƒëo√°n)
        predicted_diseases = [p['disease'] for p in predictions]
        related_diseases = []
        for disease in relevant_diseases[:15]:  # Top 15 t·ª´ database
            if disease not in predicted_diseases:
                sample_count = len(df[df['Disease'] == disease])
                # L·∫•y 1-2 tri·ªáu ch·ª©ng ƒëi·ªÉn h√¨nh
                symptoms = []
                if disease in disease_symptoms:
                    for s in disease_symptoms[disease][:2]:
                        clean = re.sub(r'T√¥i c√≥ th·ªÉ ƒëang b·ªã b·ªánh g√¨\?|"', '', s).strip()
                        clean = re.sub(r'^(T√¥i|B·ªánh nh√¢n)\s+(ƒëang|hi·ªán ƒëang|c·∫£m th·∫•y|hay b·ªã|b·ªã)\s+', '', clean)
                        if clean and len(clean) > 10:
                            symptoms.append(clean[:80])  # Limit length
                
                related_diseases.append({
                    'disease': disease,
                    'sample_symptoms': symptoms[:2],
                    'database_samples': sample_count
                })
                if len(related_diseases) >= 5:  # Ch·ªâ l·∫•y 5 b·ªánh li√™n quan
                    break
        
        # Final result v·ªõi th√¥ng tin ƒë·∫ßy ƒë·ªß
        result = {
            'success': True,
            'analysis': analysis,
            'predictions': detailed_predictions if detailed_predictions else [
                {
                    'disease': 'Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c',
                    'probability': 0,
                    'reason': 'Vui l√≤ng m√¥ t·∫£ tri·ªáu ch·ª©ng chi ti·∫øt h∆°n',
                    'typical_symptoms': [],
                    'database_samples': 0,
                    'has_database_info': False
                }
            ],
            'disease_info': disease_info,  # Th√¥ng tin chi ti·∫øt v·ªÅ b·ªánh (tri·ªáu ch·ª©ng ƒë·∫ßy ƒë·ªß, c√°ch ch·ªØa, nguy√™n nh√¢n)
            'recommendations': recommendations,
            'ai_engine': ai_engine,  # AI engine being used
            'warning': 'ƒê√¢y l√† d·ª± ƒëo√°n AI, KH√îNG PH·∫¢I ch·∫©n ƒëo√°n y t·∫ø. H√£y ƒëi kh√°m b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c!',
            
            # Th√¥ng tin b·ªï sung t·ª´ database
            'additional_info': {
                'related_diseases': related_diseases,
                'total_diseases_analyzed': len(relevant_diseases),
                'confidence_level': 'cao' if (detailed_predictions and detailed_predictions[0]['probability'] >= 70) else 'trung b√¨nh' if (detailed_predictions and detailed_predictions[0]['probability'] >= 50) else 'th·∫•p'
            },
            
            # Metadata
            'metadata': {
                'source': f'{ai_engine.upper()} AI + CSV Database',
                'model': GROQ_MODEL if ai_engine == 'groq' else GEMINI_MODEL,
                'provider': ai_engine.title(),
                'database_stats': {
                    'total_diseases': len(diseases),
                    'total_symptom_samples': len(df),
                    'diseases_searched': len(relevant_diseases)
                }
            }
        }
        
        # L∆∞u l·ªãch s·ª≠ v√†o database
        if detailed_predictions and len(detailed_predictions) > 0:
            disease_name = detailed_predictions[0]['disease']
            confidence = detailed_predictions[0].get('probability', 0)
            save_search_history(symptoms, disease_name, analysis, confidence)
        
        return jsonify(result)
        
    except Exception as e:
        error_msg = str(e)
        
        # Check for quota exceeded or rate limit
        if '429' in error_msg or 'quota' in error_msg.lower() or 'rate_limit' in error_msg.lower():
            # Fallback to CSV prediction when API fails
            print("‚ö†Ô∏è API limit reached, falling back to CSV prediction")
            relevant_context, relevant_diseases, best_match_score, top_diseases_with_scores = find_relevant_diseases(symptoms, top_k=20)
            
            if top_diseases_with_scores and len(top_diseases_with_scores) >= 3:
                result = predict_from_csv_data(symptoms, top_diseases_with_scores, ai_engine='groq')  # Fallback to groq
                if result:
                    result['warning'] = '‚ö†Ô∏è API h·∫øt quota - K·∫øt qu·∫£ t·ª´ CSV database. H√£y ƒëi kh√°m b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c!'
                    result['source'] = 'CSV Database (API unavailable)'
                    result['ai_engine'] = 'csv_fallback'
                    return jsonify(result)
            
            return jsonify({
                'error': '‚ùå H·∫æT QUOTA/RATE LIMIT API',
                'analysis': f'API key ƒë√£ h·∫øt quota ho·∫∑c v∆∞·ª£t rate limit',
                'predictions': [
                    {'disease': 'Kh√¥ng th·ªÉ d·ª± ƒëo√°n', 'probability': 0, 'reason': 'API key h·∫øt quota/rate limit'}
                ],
                'recommendations': [
                    '‚è∞ Ch·ªù m·ªôt ch√∫t ƒë·ªÉ rate limit reset',
                    'üí≥ Ki·ªÉm tra quota t·∫°i: https://console.groq.com',
                    'üîë T·∫°o API key m·ªõi n·∫øu c·∫ßn',
                    'üíª Groq c√≥ free tier r·∫•t generous'
                ],
                'warning': '‚ö†Ô∏è H·∫æT QUOTA/RATE LIMIT - Vui l√≤ng ƒë·ª£i ho·∫∑c ki·ªÉm tra API key',
                'source': 'Error'
            }), 429
        
        # Other errors - fallback to CSV
        print(f"ERROR: {error_msg}")
        print("‚ö†Ô∏è Error occurred, falling back to CSV prediction")
        
        try:
            relevant_context, relevant_diseases, best_match_score, top_diseases_with_scores = find_relevant_diseases(symptoms, top_k=20)
            
            if top_diseases_with_scores and len(top_diseases_with_scores) >= 3:
                result = predict_from_csv_data(symptoms, top_diseases_with_scores, ai_engine='groq')  # Fallback to groq
                if result:
                    result['warning'] = f'‚ö†Ô∏è API l·ªói - K·∫øt qu·∫£ t·ª´ CSV database. H√£y ƒëi kh√°m b√°c sƒ©!'
                    result['source'] = 'CSV Database (API error fallback)'
                    result['ai_engine'] = 'csv_fallback'
                    return jsonify(result)
        except Exception as csv_error:
            print(f"CSV fallback also failed: {csv_error}")
        
        return jsonify({
            'error': f'L·ªói: {error_msg[:100]}',
            'analysis': 'C√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω y√™u c·∫ßu',
            'predictions': [
                {'disease': 'L·ªói h·ªá th·ªëng', 'probability': 0, 'reason': 'Vui l√≤ng th·ª≠ l·∫°i sau'}
            ],
            'recommendations': [
                'Ki·ªÉm tra k·∫øt n·ªëi internet',
                'Th·ª≠ l·∫°i sau v√†i gi√¢y',
                'Ki·ªÉm tra console ƒë·ªÉ xem chi ti·∫øt l·ªói'
            ],
            'warning': '‚ö†Ô∏è C√≥ l·ªói x·∫£y ra - vui l√≤ng th·ª≠ l·∫°i',
            'source': 'Error'
        }), 500

@app.route('/stats')
def stats():
    """
    Endpoint tr·∫£ v·ªÅ th·ªëng k√™ chi ti·∫øt v·ªÅ database v√† system
    """
    # T√≠nh to√°n th·ªëng k√™
    disease_sample_counts = df['Disease'].value_counts().to_dict()
    top_10_diseases = dict(list(disease_sample_counts.items())[:10])
    
    # T√≠nh avg samples per disease
    avg_samples = len(df) / len(diseases)
    
    return jsonify({
        'success': True,
        'database': {
            'total_diseases': len(diseases),
            'total_symptom_samples': len(df),
            'avg_samples_per_disease': round(avg_samples, 2),
            'top_10_diseases_by_samples': top_10_diseases,
            'diseases_list_sample': diseases[:20]  # 20 b·ªánh ƒë·∫ßu ti√™n
        },
        'model': {
            'engines': {
                'groq': GROQ_MODEL,
                'gemini': GEMINI_MODEL
            },
            'default': DEFAULT_AI_ENGINE,
            'type': 'Dual AI Engine with TF-IDF + CSV Database',
            'features': [
                'TF-IDF based disease matching',
                'Database-driven symptom analysis',
                'Context-aware prediction',
                'Typical symptoms from real data'
            ]
        },
        'api': {
            'version': '2.0',
            'endpoints': {
                '/predict': 'POST - D·ª± ƒëo√°n b·ªánh t·ª´ tri·ªáu ch·ª©ng',
                '/stats': 'GET - Th·ªëng k√™ h·ªá th·ªëng',
                '/': 'GET - Giao di·ªán web'
            },
            'response_fields': [
                'analysis',
                'predictions (with typical_symptoms)',
                'recommendations',
                'additional_info (related_diseases)',
                'metadata (database_stats)'
            ]
        },
        'accuracy': {
            'estimated': '85-95%',
            'notes': 'D·ª±a tr√™n 23,520 m·∫´u tri·ªáu ch·ª©ng th·ª±c t·∫ø',
            'confidence_levels': {
                'high': 'probability >= 70%',
                'medium': '50% <= probability < 70%',
                'low': 'probability < 50%'
            }
        }
    })

@app.route('/diseases')
def get_diseases():
    """
    Endpoint tr·∫£ v·ªÅ danh s√°ch t·∫•t c·∫£ c√°c b·ªánh
    """
    disease_list = []
    for disease in sorted(diseases):
        sample_count = len(df[df['Disease'] == disease])
        disease_list.append({
            'disease': disease,
            'sample_count': sample_count
        })
    
    return jsonify({
        'success': True,
        'total': len(disease_list),
        'diseases': disease_list
    })

@app.route('/disease/<disease_name>')
def get_disease_info(disease_name):
    """
    Endpoint tr·∫£ v·ªÅ th√¥ng tin chi ti·∫øt v·ªÅ m·ªôt b·ªánh c·ª• th·ªÉ
    """
    if disease_name not in diseases:
        return jsonify({
            'success': False,
            'error': f'B·ªánh "{disease_name}" kh√¥ng c√≥ trong database'
        }), 404
    
    # L·∫•y t·∫•t c·∫£ tri·ªáu ch·ª©ng c·ªßa b·ªánh n√†y
    disease_data = df[df['Disease'] == disease_name]['Question'].tolist()
    
    # Clean symptoms
    symptoms = []
    for symptom in disease_data[:20]:  # Top 20 tri·ªáu ch·ª©ng
        clean = symptom.replace("T√¥i c√≥ th·ªÉ ƒëang b·ªã b·ªánh g√¨?", "")
        clean = clean.replace('"', '').strip()
        clean = re.sub(r'^(T√¥i|B·ªánh nh√¢n)\s+(ƒëang|hi·ªán ƒëang|c·∫£m th·∫•y|hay b·ªã|b·ªã)\s+', '', clean)
        clean = re.sub(r'^\s*c√≥ c√°c tri·ªáu ch·ª©ng nh∆∞\s+', '', clean)
        if clean and len(clean) > 10:
            symptoms.append(clean)
    
    return jsonify({
        'success': True,
        'disease': disease_name,
        'total_samples': len(disease_data),
        'typical_symptoms': symptoms[:10],  # Top 10
        'all_symptom_variations': len(disease_data)
    })

@app.route('/history', methods=['GET'])
def get_history():
    """L·∫•y danh s√°ch l·ªãch s·ª≠ t√¨m ki·∫øm"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'Database connection failed'}), 500
        
        cursor = conn.cursor(dictionary=True)
        
        # L·∫•y 50 l·ªãch s·ª≠ g·∫ßn nh·∫•t
        query = """
            SELECT id, symptoms, disease, analysis, confidence, created_at
            FROM search_history
            ORDER BY created_at DESC
            LIMIT 50
        """
        cursor.execute(query)
        history = cursor.fetchall()
        
        # Convert datetime to string
        for item in history:
            if item['created_at']:
                item['created_at'] = item['created_at'].isoformat()
        
        return jsonify({
            'success': True,
            'history': history
        })
        
    except Error as e:
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()

@app.route('/history/<int:history_id>', methods=['GET'])
def get_history_item(history_id):
    """L·∫•y chi ti·∫øt 1 l·ªãch s·ª≠"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'Database connection failed'}), 500
        
        cursor = conn.cursor(dictionary=True)
        query = "SELECT * FROM search_history WHERE id = %s"
        cursor.execute(query, (history_id,))
        item = cursor.fetchone()
        
        if item:
            if item['created_at']:
                item['created_at'] = item['created_at'].isoformat()
            return jsonify({'success': True, 'data': item})
        else:
            return jsonify({'success': False, 'error': 'Not found'}), 404
            
    except Error as e:
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()

@app.route('/history/delete/<int:history_id>', methods=['DELETE'])
def delete_history_item(history_id):
    """X√≥a 1 l·ªãch s·ª≠"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'Database connection failed'}), 500
        
        cursor = conn.cursor()
        query = "DELETE FROM search_history WHERE id = %s"
        cursor.execute(query, (history_id,))
        conn.commit()
        
        if cursor.rowcount > 0:
            return jsonify({'success': True, 'message': 'Deleted successfully'})
        else:
            return jsonify({'success': False, 'error': 'Not found'}), 404
            
    except Error as e:
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()

@app.route('/history/clear', methods=['DELETE'])
def clear_all_history():
    """X√≥a to√†n b·ªô l·ªãch s·ª≠"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'Database connection failed'}), 500
        
        cursor = conn.cursor()
        query = "DELETE FROM search_history"
        cursor.execute(query)
        conn.commit()
        
        deleted_count = cursor.rowcount
        
        return jsonify({
            'success': True,
            'message': f'Cleared {deleted_count} records',
            'count': deleted_count
        })
            
    except Error as e:
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn and conn.is_connected():
            cursor.close()
            conn.close()

if __name__ == '__main__':
    print("="*70)
    print("üåê SMART DISEASE DIAGNOSIS WEB APP")
    print("="*70)
    print(f"üìä Database: {len(diseases)} lo·∫°i b·ªánh, {len(df)} m·∫´u tri·ªáu ch·ª©ng")
    print(f"ü§ñ AI Engines: Groq ({GROQ_MODEL}) + Gemini ({GEMINI_MODEL})")
    print(f"‚ö° Strategy: CSV First ‚Üí Groq API Fallback")
    print(f"üéØ CSV Threshold: {CSV_CONFIDENCE_THRESHOLD} (ƒëi·ªÅu ch·ªânh ƒë·ªÉ t·ªëi ∆∞u)")
    print(f"\nüí° L·ª£i √≠ch:")
    print(f"   ‚Ä¢ Nhanh: D·ª± ƒëo√°n t·ª´ CSV kh√¥ng c·∫ßn g·ªçi API")
    print(f"   ‚Ä¢ Ti·∫øt ki·ªám: Gi·∫£m API calls khi c√≥ k·∫øt qu·∫£ t·ªët t·ª´ CSV")
    print(f"   ‚Ä¢ Ch√≠nh x√°c: D√πng Groq AI khi c·∫ßn ph√¢n t√≠ch ph·ª©c t·∫°p")
    # Get port from environment variable (Railway/Heroku sets this automatically)
    port = int(os.getenv('PORT', 5000))
    debug = os.getenv('FLASK_DEBUG', 'False').lower() == 'true'
    
    print(f"\nüöÄ Starting server...")
    print(f"üìç URL: http://0.0.0.0:{port}")
    print(f"üîß Debug mode: {debug}")
    print(f"\n‚ö†Ô∏è  Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng server")
    print("="*70)
    
    app.run(debug=debug, host='0.0.0.0', port=port)

