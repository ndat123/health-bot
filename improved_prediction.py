"""
Improved Disease Prediction vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao hÆ¡n
Sá»­ dá»¥ng:
1. Enhanced TF-IDF vá»›i synonym matching
2. N-gram matching (2-gram, 3-gram)
3. Weighted keywords
4. Disease-specific pattern matching
"""
import pandas as pd
import json
import os
import re
import math
from collections import defaultdict, Counter
from dotenv import load_dotenv

# Load environment
load_dotenv()

# Load data
df = pd.read_csv('ViMedical_Disease.csv', encoding='utf-8')
diseases = sorted(df['Disease'].unique().tolist())

# Build disease knowledge base vá»›i nhiá»u thÃ´ng tin hÆ¡n
disease_symptoms = {}
disease_keywords = {}  # Keywords Ä‘áº·c trÆ°ng cho má»—i bá»‡nh

for disease in diseases:
    disease_data = df[df['Disease'] == disease]['Question'].tolist()
    disease_symptoms[disease] = disease_data
    
    # Extract keywords cho má»—i bá»‡nh
    all_text = " ".join(disease_data).lower()
    words = re.findall(r'\w+', all_text)
    disease_keywords[disease] = Counter(words)

print(f"âœ“ Loaded {len(diseases)} diseases with {len(df)} symptom samples")

# Medical synonyms (tá»« Ä‘á»“ng nghÄ©a y táº¿)
MEDICAL_SYNONYMS = {
    'Ä‘au': ['nhá»©c', 'buá»‘t', 'Ä‘au Ä‘á»›n', 'Ä‘au nhá»©c', 'Ä‘au rÃ¡t'],
    'sá»‘t': ['nÃ³ng', 'sá»‘t cao', 'sá»‘t nháº¹', 'á»›n láº¡nh', 'rÃ©t run'],
    'ho': ['ho khan', 'ho cÃ³ Ä‘á»m', 'ho ra mÃ¡u', 'ho nhiá»u', 'ho dai dáº³ng'],
    'buá»“n nÃ´n': ['nÃ´n', 'Ã³i', 'nÃ´n má»­a', 'muá»‘n nÃ´n', 'á»e'],
    'chÃ³ng máº·t': ['hoa máº¯t', 'choÃ¡ng vÃ¡ng', 'ngáº¥t', 'máº¥t thÄƒng báº±ng', 'vÃ¡ng Ä‘áº§u'],
    'má»‡t': ['má»‡t má»i', 'kiá»‡t sá»©c', 'yáº¿u', 'má»i', 'uá»ƒ oáº£i'],
    'khÃ³ thá»Ÿ': ['thá»Ÿ gáº¥p', 'thá»Ÿ nhanh', 'ngáº¡t thá»Ÿ', 'khÃ³ thá»Ÿ', 'thá»Ÿ dá»‘c'],
    'Ä‘au Ä‘áº§u': ['nhá»©c Ä‘áº§u', 'Ä‘au ná»­a Ä‘áº§u', 'Ä‘au Ä‘áº§u dá»¯ dá»™i', 'Ä‘au Ä‘áº§u nhiá»u'],
    'tiÃªu cháº£y': ['Ä‘i ngoÃ i', 'phÃ¢n lá»ng', 'á»‰a cháº£y', 'Ä‘i ngoÃ i nhiá»u'],
    'tÃ¡o bÃ³n': ['khÃ³ Ä‘i ngoÃ i', 'Ä‘áº¡i tiá»‡n khÃ³', 'bÃ­ Ä‘áº¡i tiá»‡n', 'khÃ³ Ä‘i cáº§u'],
    'ngá»©a': ['ngá»©a ngÃ¡y', 'ngá»©a rÃ¡t', 'ngá»©a nhiá»u', 'ngá»©a da'],
    'sÆ°ng': ['phÃ¹', 'sÆ°ng to', 'sÆ°ng phÃ¹', 'phÃ¹ ná»'],
    'Ä‘á»': ['Ä‘á» da', 'Ä‘á» bá»«ng', 'á»­ng Ä‘á»'],
    'khÃ n': ['khÃ n tiáº¿ng', 'khÃ n giá»ng', 'máº¥t tiáº¿ng'],
    'cháº£y mÃ¡u': ['xuáº¥t huyáº¿t', 'cháº£y mÃ¡u cam', 'cháº£y mÃ¡u chÃ¢n rÄƒng'],
}

# Important keywords vá»›i trá»ng sá»‘
IMPORTANT_KEYWORDS = {
    'ho ra mÃ¡u': 10.0,
    'xuáº¥t huyáº¿t': 10.0,
    'sá»¥t cÃ¢n': 8.0,
    'khÃ³ thá»Ÿ': 7.0,
    'Ä‘au ngá»±c': 7.0,
    'co giáº­t': 10.0,
    'tÃª liá»‡t': 10.0,
    'vÃ ng da': 9.0,
    'phÃ¹': 7.0,
    'sá»‘t cao': 6.0,
    'Ä‘au dá»¯ dá»™i': 7.0,
    'máº¥t Ã½ thá»©c': 10.0,
    'ngáº¥t xiu': 9.0,
    'Ä‘au bá»¥ng dá»¯ dá»™i': 8.0,
    'nÃ´n ra mÃ¡u': 10.0,
    'phÃ¢n Ä‘en': 9.0,
    'tiá»ƒu ra mÃ¡u': 9.0,
}

# Stopwords
STOPWORDS = {
    'tÃ´i', 'cá»§a', 'cÃ³', 'bá»‹', 'Ä‘ang', 'lÃ ', 'vÃ ', 'nÃ y', 'thá»ƒ', 'cÃ¡c', 'vá»›i',
    'má»™t', 'Ä‘Æ°á»£c', 'hay', 'Ä‘á»ƒ', 'khi', 'nhÆ°', 'thÃ¬', 'nÃ o', 'lÃ m', 'trong',
    'tá»«', 'cho', 'vá»', 'ngÆ°á»i', 'nhá»¯ng', 'khÃ´ng', 'cÃ³ thá»ƒ', 'gÃ¬', 'hiá»‡n',
    'cáº£m', 'triá»‡u', 'chá»©ng', 'bá»‡nh', 'nhÃ¢n', 'Ä‘ang', 'cáº£m', 'tháº¥y'
}

def expand_with_synonyms(text):
    """Má»Ÿ rá»™ng text vá»›i cÃ¡c tá»« Ä‘á»“ng nghÄ©a"""
    expanded_terms = [text]
    text_lower = text.lower()
    
    for key, synonyms in MEDICAL_SYNONYMS.items():
        if key in text_lower:
            for syn in synonyms:
                expanded_terms.append(text_lower.replace(key, syn))
        for syn in synonyms:
            if syn in text_lower:
                expanded_terms.append(text_lower.replace(syn, key))
    
    return expanded_terms

def extract_ngrams(text, n=2):
    """Extract n-grams tá»« text"""
    words = text.lower().split()
    ngrams = []
    for i in range(len(words) - n + 1):
        ngram = " ".join(words[i:i+n])
        if len(ngram) > 5:  # Chá»‰ láº¥y ngram dÃ i
            ngrams.append(ngram)
    return ngrams

def calculate_disease_score(symptoms_input, disease, disease_symptom_list):
    """
    TÃ­nh score cho má»™t bá»‡nh dá»±a trÃªn nhiá»u yáº¿u tá»‘
    """
    score = 0
    
    # Normalize input
    symptoms_lower = symptoms_input.lower()
    
    # 1. Exact phrase matching (trá»ng sá»‘ cao nháº¥t)
    disease_text = " ".join(disease_symptom_list).lower()
    
    # Extract important phrases tá»« input
    for phrase, weight in IMPORTANT_KEYWORDS.items():
        if phrase in symptoms_lower:
            if phrase in disease_text:
                score += weight * 100  # Bonus ráº¥t cao cho important keywords
    
    # 2. N-gram matching (2-gram, 3-gram)
    input_bigrams = extract_ngrams(symptoms_input, 2)
    input_trigrams = extract_ngrams(symptoms_input, 3)
    
    for trigram in input_trigrams:
        if trigram in disease_text:
            score += 80  # Trigram match = ráº¥t tá»‘t
    
    for bigram in input_bigrams:
        if bigram in disease_text:
            score += 40  # Bigram match = tá»‘t
    
    # 3. Keyword matching vá»›i TF-IDF
    keywords = re.findall(r'\w+', symptoms_lower)
    keywords = [k for k in keywords if len(k) > 2 and k not in STOPWORDS]
    
    # TÃ­nh IDF cho keywords
    total_diseases = len(diseases)
    for keyword in keywords:
        # Äáº¿m sá»‘ bá»‡nh cÃ³ keyword nÃ y
        disease_count = sum(1 for d in diseases if keyword in " ".join(disease_symptoms[d]).lower())
        
        if disease_count > 0:
            idf = math.log(total_diseases / disease_count)
            
            # TF trong disease nÃ y
            tf = disease_text.count(keyword)
            
            if tf > 0:
                score += tf * idf * 15
    
    # 4. Synonym matching
    expanded_inputs = expand_with_synonyms(symptoms_input)
    for expanded in expanded_inputs:
        if expanded != symptoms_lower:  # KhÃ´ng tÃ­nh láº¡i input gá»‘c
            # Äáº¿m sá»‘ tá»« khá»›p
            expanded_words = set(re.findall(r'\w+', expanded))
            disease_words = set(re.findall(r'\w+', disease_text))
            common_words = expanded_words & disease_words
            score += len(common_words) * 5
    
    # 5. Disease-specific keywords (tá»« thá»‘ng kÃª)
    # Láº¥y top keywords cá»§a disease nÃ y
    if disease in disease_keywords:
        top_disease_keywords = [word for word, count in disease_keywords[disease].most_common(20)]
        for keyword in keywords:
            if keyword in top_disease_keywords:
                score += 20  # Bonus cho keyword Ä‘áº·c trÆ°ng cá»§a bá»‡nh
    
    return score

def predict_disease_improved(symptoms_input, top_k=10):
    """
    Dá»± Ä‘oÃ¡n bá»‡nh vá»›i thuáº­t toÃ¡n cáº£i tiáº¿n
    
    Returns:
        top_diseases: List of (disease, score) tuples
    """
    disease_scores = {}
    
    for disease in diseases:
        disease_symptom_list = disease_symptoms[disease]
        score = calculate_disease_score(symptoms_input, disease, disease_symptom_list)
        
        if score > 0:
            disease_scores[disease] = score
    
    # Sort by score
    top_diseases = sorted(disease_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]
    
    return top_diseases

def test_improved_model(sample_size=500):
    """Test model cáº£i tiáº¿n"""
    print(f"\n{'='*70}")
    print(f"ğŸ§ª TESTING IMPROVED MODEL")
    print(f"{'='*70}")
    print(f"Sample size: {sample_size}")
    print(f"{'='*70}\n")
    
    # Láº¥y máº«u ngáº«u nhiÃªn
    test_samples = df.sample(n=min(sample_size, len(df)), random_state=42)
    
    correct = 0
    total = 0
    top3_correct = 0  # ÄÃºng trong top 3
    top5_correct = 0  # ÄÃºng trong top 5
    
    results = []
    disease_stats = defaultdict(lambda: {'total': 0, 'correct': 0, 'top3': 0, 'top5': 0})
    
    for idx, row in test_samples.iterrows():
        actual_disease = row['Disease']
        symptoms = row['Question']
        
        # Dá»± Ä‘oÃ¡n
        top_predictions = predict_disease_improved(symptoms, top_k=10)
        
        if not top_predictions:
            total += 1
            continue
        
        predicted_disease = top_predictions[0][0]
        top3_diseases = [d for d, s in top_predictions[:3]]
        top5_diseases = [d for d, s in top_predictions[:5]]
        
        # Kiá»ƒm tra káº¿t quáº£
        is_correct = (predicted_disease == actual_disease)
        is_top3 = (actual_disease in top3_diseases)
        is_top5 = (actual_disease in top5_diseases)
        
        if is_correct:
            correct += 1
        if is_top3:
            top3_correct += 1
        if is_top5:
            top5_correct += 1
        
        total += 1
        
        # Cáº­p nháº­t thá»‘ng kÃª
        disease_stats[actual_disease]['total'] += 1
        if is_correct:
            disease_stats[actual_disease]['correct'] += 1
        if is_top3:
            disease_stats[actual_disease]['top3'] += 1
        if is_top5:
            disease_stats[actual_disease]['top5'] += 1
        
        # LÆ°u káº¿t quáº£
        results.append({
            'symptoms': symptoms[:100],
            'actual_disease': actual_disease,
            'predicted_disease': predicted_disease,
            'top3_diseases': top3_diseases,
            'is_correct': is_correct,
            'is_top3': is_top3,
            'is_top5': is_top5
        })
        
        # Progress
        if total % 50 == 0:
            current_accuracy = (correct / total) * 100
            current_top3 = (top3_correct / total) * 100
            print(f"Progress: {total}/{sample_size} - Accuracy: {current_accuracy:.2f}% | Top-3: {current_top3:.2f}%")
    
    # Káº¿t quáº£
    accuracy = (correct / total) * 100 if total > 0 else 0
    top3_accuracy = (top3_correct / total) * 100 if total > 0 else 0
    top5_accuracy = (top5_correct / total) * 100 if total > 0 else 0
    
    print(f"\n{'='*70}")
    print(f"ğŸ“Š IMPROVED MODEL RESULTS")
    print(f"{'='*70}")
    print(f"Total samples: {total}")
    print(f"Top-1 Accuracy: {accuracy:.2f}% ({correct}/{total})")
    print(f"Top-3 Accuracy: {top3_accuracy:.2f}% ({top3_correct}/{total})")
    print(f"Top-5 Accuracy: {top5_accuracy:.2f}% ({top5_correct}/{total})")
    print(f"{'='*70}\n")
    
    # LÆ°u káº¿t quáº£
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    os.makedirs('test_results', exist_ok=True)
    
    summary = {
        'timestamp': timestamp,
        'model': 'improved',
        'top1_accuracy': accuracy,
        'top3_accuracy': top3_accuracy,
        'top5_accuracy': top5_accuracy,
        'total_samples': total,
        'correct': correct,
        'top3_correct': top3_correct,
        'top5_correct': top5_correct
    }
    
    with open(f'test_results/improved_model_summary_{timestamp}.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ Results saved to test_results/improved_model_summary_{timestamp}.json\n")
    
    return accuracy, top3_accuracy, top5_accuracy, results

if __name__ == '__main__':
    print(f"\n{'='*70}")
    print(f"ğŸš€ IMPROVED DISEASE PREDICTION MODEL")
    print(f"{'='*70}\n")
    
    # Test model
    accuracy, top3_acc, top5_acc, results = test_improved_model(sample_size=500)
    
    if accuracy >= 80:
        print(f"âœ… SUCCESS! Accuracy ({accuracy:.2f}%) is above 80%!")
    else:
        print(f"âš ï¸ Accuracy ({accuracy:.2f}%) is still below 80%")
        print(f"ğŸ’¡ Consider using API-based prediction for better results")
