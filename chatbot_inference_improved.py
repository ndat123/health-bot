"""
Script c·∫£i ti·∫øn ƒë·ªÉ s·ª≠ d·ª•ng chatbot y t·∫ø ƒë√£ ƒë∆∞·ª£c train
Bao g·ªìm c√°c c·∫£i thi·ªán:
- Confidence threshold
- Gi·∫£i th√≠ch d·ª± ƒëo√°n (attention visualization)
- L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i
- X·ª≠ l√Ω c√¢u h·ªèi kh√¥ng r√µ r√†ng
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import json
import os
from datetime import datetime
import numpy as np

class ImprovedMedicalChatbot:
    def __init__(self, model_path="./chatbot_model_improved", max_length=256, 
                 confidence_threshold=0.15, out_of_domain_threshold=0.10):
        """
        Kh·ªüi t·∫°o chatbot v·ªõi c√°c t√≠nh nƒÉng c·∫£i ti·∫øn
        
        Args:
            model_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn model
            max_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa input
            confidence_threshold: Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu (0-1)
            out_of_domain_threshold: Ng∆∞·ª°ng ƒë·ªÉ ph√°t hi·ªán c√¢u h·ªèi kh√¥ng li√™n quan
        """
        self.model_path = model_path
        self.max_length = max_length
        self.dynamic_max_length = True  # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh
        self.confidence_threshold = confidence_threshold
        self.out_of_domain_threshold = out_of_domain_threshold
        self.conversation_history = []
        
        # T·ª´ kh√≥a y t·∫ø ƒë·ªÉ validate
        self.medical_keywords = [
            'tri·ªáu ch·ª©ng', 'b·ªánh', 'ƒëau', 'm·ªát', 's·ªët', 'ho', 'kh√≥ th·ªü',
            'bu·ªìn n√¥n', 'ch√≥ng m·∫∑t', 'nh·ª©c ƒë·∫ßu', 'm·∫•t ng·ªß', 'ng·ª©a', 's∆∞ng'
        ]
        
        print("ƒêang load model...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.eval()
        
        # Load disease mapping
        mapping_path = f"{model_path}/disease_mapping.json"
        if not os.path.exists(mapping_path):
            # Fallback to original model path
            mapping_path = "./chatbot_model/disease_mapping.json"
        
        with open(mapping_path, "r", encoding="utf-8") as f:
            mapping = json.load(f)
            self.id_to_disease = {int(k): v for k, v in mapping["id_to_disease"].items()}
        
        print(f"‚úì ƒê√£ load model v·ªõi {len(self.id_to_disease)} lo·∫°i b·ªánh")
        print(f"‚úì Confidence threshold: {confidence_threshold:.2%}")
    
    def validate_question(self, question):
        """Validate c√¢u h·ªèi ƒë·∫ßu v√†o"""
        question = str(question).strip()
        
        if not question or len(question) < 10:
            return False, "C√¢u h·ªèi qu√° ng·∫Øn. Vui l√≤ng m√¥ t·∫£ chi ti·∫øt h∆°n v·ªÅ tri·ªáu ch·ª©ng."
        
        if len(question) > 2000:
            return False, "C√¢u h·ªèi qu√° d√†i. Vui l√≤ng r√∫t g·ªçn l·∫°i."
        
        # Ki·ªÉm tra c√≥ ch·ª©a t·ª´ kh√≥a y t·∫ø kh√¥ng
        question_lower = question.lower()
        has_medical_keyword = any(keyword in question_lower for keyword in self.medical_keywords)
        
        if not has_medical_keyword:
            return False, "C√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn tri·ªáu ch·ª©ng y t·∫ø."
        
        return True, None
    
    def is_out_of_domain(self, probabilities):
        """Ph√°t hi·ªán c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn b·ªánh"""
        max_prob = float(torch.max(probabilities).item())
        return max_prob < self.out_of_domain_threshold, max_prob
    
    def predict(self, question, top_k=3, return_details=False):
        """
        D·ª± ƒëo√°n b·ªánh t·ª´ c√¢u h·ªèi v·ªÅ tri·ªáu ch·ª©ng v·ªõi validation v√† out-of-domain detection
        
        Args:
            question: C√¢u h·ªèi v·ªÅ tri·ªáu ch·ª©ng
            top_k: S·ªë l∆∞·ª£ng b·ªánh c√≥ kh·∫£ nƒÉng cao nh·∫•t ƒë·ªÉ tr·∫£ v·ªÅ
            return_details: C√≥ tr·∫£ v·ªÅ th√¥ng tin chi ti·∫øt kh√¥ng
        
        Returns:
            List c√°c tuple (t√™n_b·ªánh, x√°c_su·∫•t) ho·∫∑c dict n·∫øu return_details=True
        """
        # Validate question
        is_valid, error_msg = self.validate_question(question)
        if not is_valid:
            if return_details:
                return {
                    'error': error_msg,
                    'is_valid': False,
                    'question': question,
                    'is_confident': False
                }
            return [], False
        
        # T√≠nh max_length ƒë·ªông n·∫øu c·∫ßn
        if self.dynamic_max_length:
            temp_tokenized = self.tokenizer(question, truncation=False, padding=False)
            actual_length = len(temp_tokenized['input_ids'])
            effective_max_length = min(actual_length + 20, self.max_length, 512)
        else:
            effective_max_length = self.max_length
        
        # Tokenize
        inputs = self.tokenizer(
            question,
            truncation=True,
            padding='max_length',
            max_length=effective_max_length,
            return_tensors='pt'
        )
        
        # Predict
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)[0]
            
            # L·∫•y attention weights n·∫øu c√≥
            attentions = None
            if hasattr(outputs, 'attentions') and outputs.attentions is not None:
                attentions = outputs.attentions
        
        # Ki·ªÉm tra out-of-domain tr∆∞·ªõc
        is_ood, max_prob_ood = self.is_out_of_domain(probabilities)
        
        # L·∫•y top_k b·ªánh c√≥ x√°c su·∫•t cao nh·∫•t
        top_probs, top_indices = torch.topk(probabilities, min(top_k, len(self.id_to_disease)))
        
        results = []
        for prob, idx in zip(top_probs, top_indices):
            disease_name = self.id_to_disease[idx.item()]
            results.append((disease_name, prob.item()))
        
        # Ki·ªÉm tra confidence
        max_confidence = results[0][1] if results else 0.0
        is_confident = max_confidence >= self.confidence_threshold
        
        if return_details:
            return {
                'predictions': results,
                'is_confident': is_confident,
                'is_out_of_domain': is_ood,
                'max_confidence': max_confidence,
                'confidence_threshold': self.confidence_threshold,
                'out_of_domain_threshold': self.out_of_domain_threshold,
                'tokens': self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0]),
                'input_text': question,
                'is_valid': True
            }
        
        return results, is_confident
    
    def get_confidence_level(self, confidence):
        """Chuy·ªÉn ƒë·ªïi confidence th√†nh m·ª©c ƒë·ªô tin c·∫≠y"""
        if confidence >= 0.7:
            return "R·∫§T CAO", "üü¢"
        elif confidence >= 0.4:
            return "CAO", "üü°"
        elif confidence >= 0.2:
            return "TRUNG B√åNH", "üü†"
        else:
            return "TH·∫§P", "üî¥"
    
    def format_prediction_output(self, results, is_confident):
        """Format output d·ª± ƒëo√°n v·ªõi m√†u s·∫Øc v√† emoji"""
        output = []
        
        if not is_confident:
            output.append("\n‚ö†Ô∏è  C·∫¢NH B√ÅO: ƒê·ªô tin c·∫≠y th·∫•p!")
            output.append(f"X√°c su·∫•t cao nh·∫•t ch·ªâ {results[0][1]*100:.2f}% (ng∆∞·ª°ng: {self.confidence_threshold*100:.1f}%)")
            output.append("Tri·ªáu ch·ª©ng c√≥ th·ªÉ kh√¥ng r√µ r√†ng ho·∫∑c kh√¥ng ƒë·ªß th√¥ng tin.\n")
        
        output.append("D·ª±a tr√™n c√°c tri·ªáu ch·ª©ng b·∫°n m√¥ t·∫£, b·∫°n c√≥ th·ªÉ ƒëang m·∫Øc:\n")
        
        for i, (disease, prob) in enumerate(results, 1):
            level, emoji = self.get_confidence_level(prob)
            bar_length = int(prob * 30)  # Progress bar
            bar = "‚ñà" * bar_length + "‚ñë" * (30 - bar_length)
            
            output.append(f"{i}. {disease}")
            output.append(f"   {emoji} X√°c su·∫•t: {prob*100:.2f}% - ƒê·ªô tin c·∫≠y: {level}")
            output.append(f"   [{bar}]\n")
        
        return "\n".join(output)
    
    def suggest_more_info(self, results):
        """ƒê·ªÅ xu·∫•t th√¥ng tin c·∫ßn b·ªï sung"""
        suggestions = []
        
        # N·∫øu confidence th·∫•p, ƒë·ªÅ xu·∫•t th√™m th√¥ng tin
        if results[0][1] < self.confidence_threshold:
            suggestions.append("\nüí° ƒê·ªÄ XU·∫§T: Vui l√≤ng cung c·∫•p th√™m th√¥ng tin:")
            suggestions.append("   - Tri·ªáu ch·ª©ng c·ª• th·ªÉ h∆°n (v·ªã tr√≠, m·ª©c ƒë·ªô, th·ªùi gian)")
            suggestions.append("   - C√°c tri·ªáu ch·ª©ng k√®m theo kh√°c")
            suggestions.append("   - Th·ªùi gian xu·∫•t hi·ªán tri·ªáu ch·ª©ng")
            suggestions.append("   - Y·∫øu t·ªë l√†m tƒÉng/gi·∫£m tri·ªáu ch·ª©ng")
        
        # N·∫øu top 2 predictions g·∫ßn nhau, c·∫£nh b√°o
        if len(results) >= 2:
            diff = results[0][1] - results[1][1]
            if diff < 0.1:  # Ch√™nh l·ªách < 10%
                suggestions.append(f"\n‚ö†Ô∏è  L∆ØU √ù: X√°c su·∫•t gi·ªØa 2 b·ªánh h√†ng ƒë·∫ßu r·∫•t g·∫ßn nhau")
                suggestions.append(f"   ({results[0][0]}: {results[0][1]*100:.1f}% vs {results[1][0]}: {results[1][1]*100:.1f}%)")
                suggestions.append("   C·∫ßn th√™m th√¥ng tin ƒë·ªÉ ph√¢n bi·ªát ch√≠nh x√°c.")
        
        return "\n".join(suggestions) if suggestions else ""
    
    def save_conversation(self, filename="conversation_history.json"):
        """L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        filepath = os.path.join(self.model_path, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        return filepath
    
    def chat(self, save_history=True):
        """Ch·∫ø ƒë·ªô chat t∆∞∆°ng t√°c c·∫£i ti·∫øn"""
        print("\n" + "=" * 70)
        print("CHATBOT Y T·∫æ TI·∫æNG VI·ªÜT - PHI√äN B·∫¢N C·∫¢I TI·∫æN")
        print("=" * 70)
        print("Nh·∫≠p c√¢u h·ªèi v·ªÅ tri·ªáu ch·ª©ng c·ªßa b·∫°n.")
        print("G√µ 'quit' ho·∫∑c 'exit' ƒë·ªÉ tho√°t.")
        print("G√µ 'history' ƒë·ªÉ xem l·ªãch s·ª≠ h·ªôi tho·∫°i.")
        print("G√µ 'clear' ƒë·ªÉ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i.")
        print("=" * 70)
        print(f"\nüìä Th√¥ng tin:")
        print(f"   - Confidence threshold: {self.confidence_threshold*100:.1f}%")
        print(f"   - S·ªë lo·∫°i b·ªánh: {len(self.id_to_disease)}")
        print(f"   - Model: {self.model_path}")
        print("\n" + "-" * 70 + "\n")
        
        session_start = datetime.now()
        
        while True:
            try:
                question = input("üßë B·∫°n: ").strip()
                
                # X·ª≠ l√Ω commands
                if question.lower() in ['quit', 'exit', 'tho√°t', 'q']:
                    if save_history and self.conversation_history:
                        filepath = self.save_conversation()
                        print(f"\nüíæ L·ªãch s·ª≠ h·ªôi tho·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {filepath}")
                    print("\nüëã C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng chatbot! H·∫πn g·∫∑p l·∫°i!")
                    break
                
                if question.lower() == 'history':
                    self.show_history()
                    continue
                
                if question.lower() == 'clear':
                    self.conversation_history = []
                    print("\n‚úì ƒê√£ x√≥a l·ªãch s·ª≠ h·ªôi tho·∫°i.\n")
                    continue
                
                if not question:
                    continue
                
                # D·ª± ƒëo√°n v·ªõi details ƒë·ªÉ ki·ªÉm tra validation
                timestamp = datetime.now()
                result_details = self.predict(question, top_k=5, return_details=True)
                
                # Ki·ªÉm tra n·∫øu c√≥ l·ªói validation
                if isinstance(result_details, dict) and not result_details.get('is_valid', True):
                    print(f"\n‚ö†Ô∏è  Chatbot: {result_details.get('error', 'C√¢u h·ªèi kh√¥ng h·ª£p l·ªá')}\n")
                    print("-" * 70 + "\n")
                    continue
                
                # L·∫•y results t·ª´ details
                if isinstance(result_details, dict):
                    results = result_details['predictions']
                    is_ood = result_details.get('is_out_of_domain', False)
                    is_confident = result_details.get('is_confident', False)
                else:
                    results = result_details
                    is_ood = False
                    is_confident = False
                
                # L∆∞u v√†o l·ªãch s·ª≠
                self.conversation_history.append({
                    'timestamp': timestamp.strftime('%Y-%m-%d %H:%M:%S'),
                    'question': question,
                    'predictions': [
                        {'disease': disease, 'probability': float(prob)}
                        for disease, prob in results
                    ],
                    'is_confident': is_confident,
                    'is_out_of_domain': is_ood,
                    'max_confidence': float(results[0][1]) if results else 0.0
                })
                
                # C·∫£nh b√°o n·∫øu out-of-domain
                if is_ood:
                    print("\n‚ö†Ô∏è  C·∫¢NH B√ÅO: C√¢u h·ªèi c√≥ th·ªÉ kh√¥ng li√™n quan ƒë·∫øn b·ªánh!")
                    if results:
                        print(f"ƒê·ªô tin c·∫≠y r·∫•t th·∫•p ({results[0][1]*100:.2f}%).")
                    print("Vui l√≤ng m√¥ t·∫£ c√°c tri·ªáu ch·ª©ng y t·∫ø c·ª• th·ªÉ.\n")
                
                # Hi·ªÉn th·ªã k·∫øt qu·∫£
                if results:
                    print("\nü§ñ Chatbot:")
                    print(self.format_prediction_output(results[:3], is_confident))
                    
                    # ƒê·ªÅ xu·∫•t th√™m th√¥ng tin n·∫øu c·∫ßn
                    suggestions = self.suggest_more_info(results)
                    if suggestions:
                        print(suggestions)
                    
                    # Hi·ªÉn th·ªã th√™m 2 b·ªánh ti·∫øp theo n·∫øu c√≥
                    if len(results) > 3:
                        print("\nüìã C√°c kh·∫£ nƒÉng kh√°c (x√°c su·∫•t th·∫•p h∆°n):")
                        for i, (disease, prob) in enumerate(results[3:5], 4):
                            print(f"   {i}. {disease} ({prob*100:.2f}%)")
                    
                    print("\n" + "‚ö†Ô∏è" * 35)
                    print("‚ö†Ô∏è  L∆ØU √ù QUAN TR·ªåNG:")
                    print("   - ƒê√¢y ch·ªâ l√† d·ª± ƒëo√°n s∆° b·ªô d·ª±a tr√™n AI, KH√îNG PH·∫¢I ch·∫©n ƒëo√°n y t·∫ø")
                    print("   - B·∫°n N√äN tham kh·∫£o √Ω ki·∫øn b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c")
                    print("   - Kh√¥ng t·ª± √Ω ƒëi·ªÅu tr·ªã d·ª±a tr√™n k·∫øt qu·∫£ n√†y")
                    print("‚ö†Ô∏è" * 35)
                    print("\n" + "-" * 70 + "\n")
                
            except KeyboardInterrupt:
                print("\n\n‚ö†Ô∏è  ƒê√£ nh·∫≠n Ctrl+C. ƒêang tho√°t...")
                if save_history and self.conversation_history:
                    filepath = self.save_conversation()
                    print(f"üíæ L·ªãch s·ª≠ h·ªôi tho·∫°i ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {filepath}")
                break
            except Exception as e:
                print(f"\n‚ùå L·ªói: {str(e)}")
                print("Vui l√≤ng th·ª≠ l·∫°i.\n")
    
    def show_history(self):
        """Hi·ªÉn th·ªã l·ªãch s·ª≠ h·ªôi tho·∫°i"""
        if not self.conversation_history:
            print("\nüì≠ Ch∆∞a c√≥ l·ªãch s·ª≠ h·ªôi tho·∫°i.\n")
            return
        
        print("\n" + "=" * 70)
        print(f"L·ªäCH S·ª¨ H·ªòI THO·∫†I ({len(self.conversation_history)} c√¢u h·ªèi)")
        print("=" * 70)
        
        for i, entry in enumerate(self.conversation_history, 1):
            print(f"\n{i}. [{entry['timestamp']}]")
            print(f"   C√¢u h·ªèi: {entry['question']}")
            print(f"   D·ª± ƒëo√°n h√†ng ƒë·∫ßu: {entry['predictions'][0]['disease']}")
            print(f"   X√°c su·∫•t: {entry['predictions'][0]['probability']*100:.2f}%")
            confidence_status = "‚úì Tin c·∫≠y" if entry['is_confident'] else "‚ö† Kh√¥ng tin c·∫≠y"
            print(f"   Tr·∫°ng th√°i: {confidence_status}")
        
        print("\n" + "=" * 70 + "\n")
    
    def batch_predict(self, questions, output_file=None):
        """
        D·ª± ƒëo√°n cho nhi·ªÅu c√¢u h·ªèi c√πng l√∫c
        
        Args:
            questions: List c√°c c√¢u h·ªèi
            output_file: File ƒë·ªÉ l∆∞u k·∫øt qu·∫£ (optional)
        
        Returns:
            List k·∫øt qu·∫£ d·ª± ƒëo√°n
        """
        results = []
        
        print(f"\nüîÑ ƒêang x·ª≠ l√Ω {len(questions)} c√¢u h·ªèi...")
        
        for i, question in enumerate(questions, 1):
            print(f"   [{i}/{len(questions)}] Processing...", end='\r')
            preds, is_confident = self.predict(question, top_k=3)
            results.append({
                'question': question,
                'predictions': [
                    {'disease': disease, 'probability': float(prob)}
                    for disease, prob in preds
                ],
                'is_confident': is_confident
            })
        
        print(f"\n‚úì Ho√†n th√†nh x·ª≠ l√Ω {len(questions)} c√¢u h·ªèi!")
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"üíæ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_file}")
        
        return results

def main():
    """H√†m main"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Chatbot Y T·∫ø Ti·∫øng Vi·ªát - Phi√™n b·∫£n c·∫£i ti·∫øn')
    parser.add_argument('--model', type=str, default='./chatbot_model_improved',
                        help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn model')
    parser.add_argument('--threshold', type=float, default=0.15,
                        help='Confidence threshold (0-1)')
    parser.add_argument('--batch', type=str, default=None,
                        help='File ch·ª©a danh s√°ch c√¢u h·ªèi ƒë·ªÉ x·ª≠ l√Ω batch')
    parser.add_argument('--output', type=str, default=None,
                        help='File output cho batch processing')
    
    args = parser.parse_args()
    
    # Ki·ªÉm tra model path
    if not os.path.exists(args.model):
        # Th·ª≠ fallback sang model c≈©
        if os.path.exists('./chatbot_model'):
            print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y model t·∫°i {args.model}")
            print("   S·ª≠ d·ª•ng model c≈© t·∫°i ./chatbot_model")
            args.model = './chatbot_model'
        else:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y model t·∫°i {args.model}")
            print("Vui l√≤ng ch·∫°y train_chatbot_improved.py tr∆∞·ªõc ƒë·ªÉ train model.")
            return
    
    # Kh·ªüi t·∫°o chatbot
    chatbot = ImprovedMedicalChatbot(
        model_path=args.model,
        confidence_threshold=args.threshold
    )
    
    # Batch processing ho·∫∑c interactive chat
    if args.batch:
        if not os.path.exists(args.batch):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {args.batch}")
            return
        
        with open(args.batch, 'r', encoding='utf-8') as f:
            questions = [line.strip() for line in f if line.strip()]
        
        output_file = args.output or 'batch_predictions.json'
        chatbot.batch_predict(questions, output_file)
    else:
        # Ch·∫°y ch·∫ø ƒë·ªô chat
        chatbot.chat()

if __name__ == "__main__":
    main()

